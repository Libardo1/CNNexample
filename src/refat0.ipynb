{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all imports\n",
    "from __future__ import print_function\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from six.moves import range\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and reshape it in a 4 dimension tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28, 1) (200000, 10)\n",
      "Validation: (10000, 28, 28, 1) (10000, 10)\n",
      "Testing: (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save\n",
    "\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "# reformating the dataset to be a 4 dimension tensor\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f32904e5438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "#getting the classes from the one-hot-labels\n",
    "train_classes = np.argmax(train_labels, axis=1)\n",
    "train_classes = [chr(i +ord('A')) for i in train_classes]\n",
    "\n",
    "# the images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Get the first images from the test-set.\n",
    "images = train_dataset[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = train_classes[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)  \n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to creat a convolutional layer with max pooling\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model: \n",
    "---\n",
    "2 convolutional layers and 4 connected layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow graph\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 6\n",
    "num_filters_1 = 16\n",
    "num_filters_2 = 32\n",
    "hidden_nodes_1 = 60\n",
    "hidden_nodes_2 = 40\n",
    "hidden_nodes_3 = 20\n",
    "\n",
    "class CNNModel:\n",
    "     \n",
    "    def __init__(self):\n",
    "            \"\"\"\n",
    "            init\n",
    "            \"\"\"\n",
    "            self.build_graph()\n",
    "\n",
    "    def create_placeholders(self):           \n",
    "        shape_X = (batch_size, image_size,image_size, num_channels)\n",
    "        shape_Y =(batch_size, num_labels)\n",
    "        name_X = 'X'\n",
    "        name_Y = 'Y'\n",
    "        self.tf_train_dataset = tf.placeholder(tf.float32, shape =shape_X ,name = name_X)\n",
    "        self.tf_train_labels = tf.placeholder(tf.float32, shape=shape_Y, name = name_Y)\n",
    "\n",
    "    def init_weights_biases(self,shape,weights_name,biases_name):\n",
    "        layer = {\\\n",
    "            'weights':tf.Variable(tf.truncated_normal(shape, stddev=0.1),name=weights_name),\n",
    "            'biases':tf.Variable(tf.zeros(shape[-1]),name=biases_name)}\n",
    "        return layer\n",
    "\n",
    "    def create_all_weights_bias(self):\n",
    "            shape1 = [patch_size,\n",
    "                      patch_size,\n",
    "                      num_channels,\n",
    "                      num_filters_1]\n",
    "            self.conv_layer_1_wb = self.init_weights_biases(shape1,\n",
    "                                                  \"weights1\",\n",
    "                                                  \"biases1\")\n",
    "\n",
    "            shape2 = [patch_size,\n",
    "                      patch_size,\n",
    "                      num_filters_1,\n",
    "                      num_filters_2] \n",
    "            self.conv_layer_2_wb = self.init_weights_biases(shape2,\n",
    "                                                  \"weights2\",\n",
    "                                                  \"biases2\")\n",
    "\n",
    "            shape3 = [image_size // 4 * image_size // 4 * num_filters_2,\n",
    "                      hidden_nodes_1] \n",
    "            self.fully_hidden_layer_1 = self.init_weights_biases(shape3,\n",
    "                                                       \"weights3\",\n",
    "                                                       'biases3')\n",
    "            shape4 = [hidden_nodes_1, hidden_nodes_2]\n",
    "            self.fully_hidden_layer_2 = self.init_weights_biases(shape4,\n",
    "                                                       \"weights4\",\n",
    "                                                       'biases4')\n",
    "\n",
    "            shape5 = [hidden_nodes_2, hidden_nodes_3] \n",
    "            self.fully_hidden_layer_3= self.init_weights_biases(shape5,\n",
    "                                                      \"weights5\",\n",
    "                                                      'biases5')\n",
    "            shape6 = [hidden_nodes_3, num_labels]\n",
    "            self.fully_hidden_layer_4 = self.init_weights_biases(shape6,\n",
    "                                                       \"weights6\",\n",
    "                                                       'biases6')      \n",
    "    def create_summaries(self):\n",
    "        #histogram summaries for weights\n",
    "        tf.summary.histogram('weights1_summ',self.conv_layer_1_wb['weights'])\n",
    "        tf.summary.histogram('weights2_summ',self.conv_layer_2_wb['weights'])\n",
    "        tf.summary.histogram('weights3_summ',self.fully_hidden_layer_1['weights'])\n",
    "        tf.summary.histogram('weights4_summ',self.fully_hidden_layer_2['weights'])\n",
    "        tf.summary.histogram('weights5_summ',self.fully_hidden_layer_3['weights'])\n",
    "        tf.summary.histogram('weights6_summ',self.fully_hidden_layer_4['weights'])\n",
    "\n",
    "    def apply_conv(self,\n",
    "                       input_tensor,\n",
    "                       layer,\n",
    "                       use_pooling=True):  # Use 2x2 max-pooling.\n",
    "            \"\"\"\n",
    "            Create conv layer\n",
    "            \"\"\"\n",
    "\n",
    "            weights = layer['weights']\n",
    "            biases = layer['biases']\n",
    "\n",
    "            # Create the TensorFlow operation for convolution.\n",
    "            # Note the strides are set to 1 in all dimensions.\n",
    "            # The first and last stride must always be 1,\n",
    "            # because the first is for the image-number and\n",
    "            # the last is for the input-channel.\n",
    "            # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "            # is moved 2 pixels across the x- and y-axis of the image.\n",
    "            # The padding is set to 'SAME' which means the input image\n",
    "            # is padded with zeroes so the size of the output is the same.\n",
    "            conv_layer = tf.nn.conv2d(input=input_tensor,\n",
    "                                      filter=weights,\n",
    "                                      strides=[1, 1, 1, 1],\n",
    "                                      padding='SAME')\n",
    "\n",
    "            # Add the biases to the results of the convolution.\n",
    "            # A bias-value is added to each filter-channel.\n",
    "            conv_layer += biases\n",
    "            # Use pooling to down-sample the image resolution?\n",
    "            if use_pooling:\n",
    "                # This is 2x2 max-pooling, which means that we\n",
    "                # consider 2x2 windows and select the largest value\n",
    "                # in each window. Then we move 2 pixels to the next window.\n",
    "                conv_layer = tf.nn.max_pool(value=conv_layer,\n",
    "                                            ksize=[1, 2, 2, 1],\n",
    "                                            strides=[1, 2, 2, 1],\n",
    "                                            padding='SAME')\n",
    "            conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "            return conv_layer         \n",
    "        \n",
    "    def linear_activation(self,\n",
    "                          input_tensor,\n",
    "                          layer):\n",
    "        \"\"\"\n",
    "        Method to computing linear activation\n",
    "        \"\"\"\n",
    "        return tf.add(tf.matmul(input_tensor, layer['weights']),\n",
    "                      layer['biases'])\n",
    "\n",
    "    def forward_prop(self,\n",
    "                     input_tensor,\n",
    "                     V_hidden_layer_1,\n",
    "                     V_hidden_layer_2,\n",
    "                     V_hidden_layer_3,\n",
    "                     V_hidden_layer_4,\n",
    "                     V_hidden_layer_5,\n",
    "                     V_hidden_layer_6):\n",
    "        with tf.name_scope('Convolution_1'):\n",
    "            conv_layer1 = self.apply_conv(input_tensor,V_hidden_layer_1,use_pooling=True)\n",
    "        with tf.name_scope('Convolution_2'):\n",
    "            conv_layer2 = self.apply_conv(conv_layer1,V_hidden_layer_2,use_pooling=True)\n",
    "        with tf.name_scope('Reshape'):\n",
    "            shape = conv_layer2.get_shape().as_list() \n",
    "            reshape = tf.reshape(conv_layer2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        with tf.name_scope('Hidden_Layer_1'):\n",
    "            hidden_la1 = tf.nn.relu(self.linear_activation(reshape,V_hidden_layer_3)) \n",
    "        with tf.name_scope('Hidden_Layer_2'):\n",
    "            hidden_la2 = tf.sigmoid(self.linear_activation(hidden_la1,V_hidden_layer_4))\n",
    "        with tf.name_scope('Hidden_Layer_3'):\n",
    "            hidden_la3 = tf.sigmoid(self.linear_activation(hidden_la2,V_hidden_layer_5))     \n",
    "        with tf.name_scope('Output_Layer'):\n",
    "            logits = self.linear_activation(hidden_la3,V_hidden_layer_6) \n",
    "        return logits\n",
    "\n",
    "    def create_loss(self):\n",
    "        \"\"\"\n",
    "        Create the loss function of the model\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                               labels=self.tf_train_labels))\n",
    "            tf.summary.scalar(self.loss.op.name,self.loss)\n",
    " \n",
    "    def sgd_train(self,\n",
    "                  loss,\n",
    "                  starter_learning_rate,\n",
    "                  steps_for_decay,\n",
    "                  decay_rate):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                   global_step,\n",
    "                                                   steps_for_decay,\n",
    "                                                   decay_rate,\n",
    "                                                   staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Create the optimization of the model\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"train\"):\n",
    "            self.optimizer = self.sgd_train(self.loss,\n",
    "                                            0.9,\n",
    "                                            100,\n",
    "                                            0.96)         \n",
    "\n",
    "    def create_predictions(self):\n",
    "        self.train_prediction = tf.nn.softmax(self.logits, name='train_network')\n",
    "        self.train_pred_cls = tf.argmax(self.train_prediction, dimension=1)\n",
    "        self.valid_prediction = tf.nn.softmax(self.valid_network, name='valid_network')\n",
    "        self.test_prediction = tf.nn.softmax(self.test_network, name='test_network')\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.train_prediction = tf.nn.softmax(self.logits, name='train_network')\n",
    "            correct_pred = tf.equal(tf.argmax(self.train_prediction, 1), tf.argmax(self.tf_train_labels, 1))\n",
    "            self.acc_op = tf.reduce_mean(tf.cast(correct_pred,'float'))        \n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph() \n",
    "        with self.graph.as_default():\n",
    "\n",
    "            #Placeholders\n",
    "            self.create_placeholders()\n",
    "            #Constants\n",
    "            self.tf_valid_dataset = tf.constant(valid_dataset, name ='X_va')\n",
    "            self.tf_test_dataset = tf.constant(test_dataset, name ='X_test')\n",
    "            #all layers weights and biases\n",
    "            self.create_all_weights_bias()\n",
    "            self.t1 = self.conv_layer_1_wb['weights']\n",
    "            self.t2 = self.conv_layer_1_wb['biases']\n",
    "            self.tt1 = self.conv_layer_2_wb['weights']\n",
    "            self.tt2 = self.conv_layer_2_wb['biases']\n",
    "            self.tc1 = self.fully_hidden_layer_1['weights']\n",
    "            self.tc2 = self.fully_hidden_layer_1['biases']\n",
    "            self.tcc1 = self.fully_hidden_layer_2['weights']\n",
    "            self.tcc2 = self.fully_hidden_layer_2['biases']\n",
    "            self.te1 = self.fully_hidden_layer_3['weights']\n",
    "            self.te2 = self.fully_hidden_layer_3['biases']\n",
    "            self.tee1 = self.fully_hidden_layer_4['weights']\n",
    "            self.tee2 = self.fully_hidden_layer_4['biases']\n",
    "            #all summaries\n",
    "            self.create_summaries()\n",
    "            #forward propagation\n",
    "            self.logits = self.forward_prop(self.tf_train_dataset,\n",
    "                                  self.conv_layer_1_wb,\n",
    "                                  self.conv_layer_2_wb,\n",
    "                                  self.fully_hidden_layer_1,\n",
    "                                  self.fully_hidden_layer_2,\n",
    "                                  self.fully_hidden_layer_3,\n",
    "                                  self.fully_hidden_layer_4)\n",
    "            \n",
    "            self.valid_network = self.forward_prop(self.tf_valid_dataset,\n",
    "                                       self.conv_layer_1_wb,\n",
    "                                       self.conv_layer_2_wb,\n",
    "                                       self.fully_hidden_layer_1,\n",
    "                                       self.fully_hidden_layer_2,\n",
    "                                       self.fully_hidden_layer_3,\n",
    "                                       self.fully_hidden_layer_4)\n",
    "            \n",
    "            self.test_network = self.forward_prop(self.tf_test_dataset,\n",
    "                                       self.conv_layer_1_wb,\n",
    "                                       self.conv_layer_2_wb,\n",
    "                                       self.fully_hidden_layer_1,\n",
    "                                       self.fully_hidden_layer_2,\n",
    "                                       self.fully_hidden_layer_3,\n",
    "                                       self.fully_hidden_layer_4)\n",
    "            #loss\n",
    "            self.create_loss()\n",
    "            #opt\n",
    "            self.create_optimizer()\n",
    "            #predictions\n",
    "            self.create_predictions()\n",
    "            #accuracy\n",
    "            self.create_accuracy()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.11960784]\n",
      "   [-0.19803922]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.2254902 ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.07647059]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.49215686]\n",
      "   ..., \n",
      "   [-0.19019608]\n",
      "   [-0.5       ]\n",
      "   [-0.49607843]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.49215686]\n",
      "   [-0.49215686]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.48823529]\n",
      "   [-0.49607843]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.26862746]\n",
      "   [ 0.07647059]\n",
      "   [-0.15882353]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.2764706 ]\n",
      "   [-0.46470588]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.44509804]\n",
      "   [-0.37843138]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.48823529]\n",
      "   [ 0.49607843]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.3509804 ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49607843]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.24509804]\n",
      "   [ 0.34313726]\n",
      "   [ 0.47254902]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.49607843]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.11568628]\n",
      "   [-0.48431373]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [-0.05294118]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.06078431]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.08039216]\n",
      "   [-0.46078432]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.48823529]\n",
      "   ..., \n",
      "   [-0.48823529]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.30392158]\n",
      "   [-0.22156863]\n",
      "   [-0.22941177]\n",
      "   ..., \n",
      "   [-0.22941177]\n",
      "   [-0.22156863]\n",
      "   [-0.30392158]]\n",
      "\n",
      "  [[ 0.24117647]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [ 0.49215686]\n",
      "   [ 0.5       ]\n",
      "   [ 0.24117647]]]\n",
      "\n",
      "\n",
      " [[[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.48431373]\n",
      "   [ 0.48431373]\n",
      "   [ 0.46470588]]\n",
      "\n",
      "  [[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]]\n",
      "\n",
      "  [[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.48823529]\n",
      "   ..., \n",
      "   [ 0.0254902 ]\n",
      "   [ 0.0254902 ]\n",
      "   [ 0.01372549]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.48431373]\n",
      "   ..., \n",
      "   [-0.10392157]\n",
      "   [-0.10392157]\n",
      "   [-0.10784314]]\n",
      "\n",
      "  [[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]]\n",
      "\n",
      "  [[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.48823529]\n",
      "   [ 0.48823529]\n",
      "   [ 0.46862745]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.10392157]\n",
      "   [-0.28431374]\n",
      "   [-0.48823529]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.46470588]\n",
      "   [-0.2647059 ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.05686275]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.49607843]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.49215686]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]]\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "[[[[ -1.35859344e-02  -1.53722048e-01   2.15443950e-02   8.34949389e-02\n",
      "     -9.81626008e-03   6.40858933e-02   6.93955272e-02  -8.99999812e-02\n",
      "      5.85042015e-02  -1.01226019e-02   4.18142043e-02  -3.04521229e-02\n",
      "      1.02806687e-01   1.38544580e-02   1.55622631e-01   1.07748900e-02]]\n",
      "\n",
      "  [[  1.17509840e-02  -8.52671042e-02  -9.09451395e-02   9.30553898e-02\n",
      "     -1.02892136e-02  -2.46885344e-02   1.21411728e-02  -3.52536119e-03\n",
      "     -1.16322838e-01   1.54954061e-01   1.13061048e-01  -9.06482115e-02\n",
      "     -2.25090403e-02  -3.51594314e-02  -4.99508344e-02  -1.89202070e-01]]\n",
      "\n",
      "  [[ -3.85534167e-02   7.47317821e-02  -1.57434791e-01   5.46358302e-02\n",
      "      3.20719443e-02   2.64866184e-03   4.56136726e-02  -3.81891653e-02\n",
      "     -2.08165627e-02  -4.47180383e-02   1.29007682e-01  -1.27598450e-01\n",
      "     -3.17302160e-02   1.15083624e-02  -7.12312832e-02   1.20706800e-02]]\n",
      "\n",
      "  [[  1.72930107e-01  -2.34168023e-02   9.23299640e-02  -2.81640887e-02\n",
      "      7.34719932e-02   1.08356671e-02  -3.82089801e-02   3.79931703e-02\n",
      "     -6.02599494e-02  -1.02733769e-01  -8.12710077e-02  -1.75570756e-01\n",
      "     -1.99869022e-01   8.36853012e-02  -4.88498360e-02   1.99189559e-01]]\n",
      "\n",
      "  [[  1.44677803e-01  -8.64424929e-02   4.63661440e-02   8.35261568e-02\n",
      "      4.31346372e-02  -1.16716161e-01   1.25042126e-02   3.55037116e-02\n",
      "      1.55192420e-01  -9.44970474e-02   4.16906886e-02  -3.53063904e-02\n",
      "     -6.28909320e-02   8.65687057e-02   3.19245309e-02  -5.49011938e-02]]\n",
      "\n",
      "  [[  1.19357847e-01  -6.72560409e-02   8.28074291e-02  -5.47155552e-02\n",
      "     -9.01725292e-02  -6.40656427e-02   5.32793701e-02  -1.03236593e-01\n",
      "      1.11913227e-01  -1.28561705e-01  -5.12674116e-02   1.45382121e-01\n",
      "      8.94650817e-03  -2.34486046e-03  -1.59371179e-02   7.10605457e-02]]]\n",
      "\n",
      "\n",
      " [[[  4.99611087e-02  -9.72121730e-02  -1.05335712e-01   9.10937786e-02\n",
      "     -6.19698130e-02  -1.27691254e-01  -1.16455339e-01   1.50266171e-01\n",
      "      1.17943287e-01  -8.92241523e-02  -7.79915005e-02   9.20864120e-02\n",
      "      7.60525838e-02  -1.54505894e-01   2.84219179e-02  -3.30980793e-02]]\n",
      "\n",
      "  [[  8.69010687e-02   1.23805203e-01  -1.23663209e-01   3.70092429e-02\n",
      "      7.82021955e-02  -1.10509768e-01   4.93644997e-02  -7.80823454e-02\n",
      "     -3.02524306e-02  -1.88663647e-01  -9.05191060e-03   3.96818779e-02\n",
      "      9.03005432e-03  -4.54360954e-02   9.08191204e-02   7.20098754e-03]]\n",
      "\n",
      "  [[  7.86216483e-02  -9.15295407e-02   6.33419901e-02  -4.61951457e-02\n",
      "     -9.09218844e-03  -7.43223280e-02   1.30447997e-02   9.36546057e-05\n",
      "     -3.36204395e-02   6.44987971e-02  -1.39138564e-01  -7.83388969e-03\n",
      "      1.38455361e-01   4.60248999e-02  -1.16418660e-01   9.52316150e-02]]\n",
      "\n",
      "  [[ -1.56109840e-01  -1.00845415e-02   1.32226393e-01   1.04210034e-01\n",
      "      1.56673476e-01   5.50323687e-02  -1.32505819e-01   1.84321433e-01\n",
      "     -7.93750212e-02  -5.57978749e-02  -6.19101003e-02  -3.46658975e-02\n",
      "      9.31276008e-02  -8.85451213e-02  -1.67052716e-03  -7.15378299e-03]]\n",
      "\n",
      "  [[ -3.83284502e-02   6.46570176e-02  -1.13893770e-01   9.68819410e-02\n",
      "      2.27539241e-02  -1.96755931e-01  -4.89884131e-02   2.14088038e-02\n",
      "      1.07752696e-01   1.78118512e-01  -1.13095336e-01  -4.06318642e-02\n",
      "     -7.82741234e-02  -3.24092768e-02  -1.10820420e-02  -7.41507262e-02]]\n",
      "\n",
      "  [[  1.26267197e-02   1.50033519e-01  -1.04817860e-01  -1.45266224e-02\n",
      "      1.04227185e-01   3.31898741e-02  -9.01217461e-02  -1.74527243e-01\n",
      "      8.57963786e-03  -3.55102867e-02  -1.42607018e-01  -3.44808996e-02\n",
      "      3.57900187e-02  -1.48062915e-01   8.44765455e-03  -5.68090752e-02]]]\n",
      "\n",
      "\n",
      " [[[  6.26210496e-02   8.13553557e-02   1.40414819e-01   3.96317653e-02\n",
      "      4.59470749e-02   9.42005515e-02  -3.47405374e-02  -4.25140448e-02\n",
      "     -1.75125346e-01   5.83119504e-02  -1.07557766e-01  -1.70762818e-02\n",
      "      9.68014449e-02  -1.84569597e-01   2.70910352e-03   9.82002169e-02]]\n",
      "\n",
      "  [[ -5.68806008e-02  -1.44743845e-01  -1.06766103e-02  -6.82513863e-02\n",
      "     -6.05678447e-02   8.37399065e-02   6.26650602e-02  -1.52644619e-01\n",
      "      7.21025169e-02  -1.08659230e-01   1.49979487e-01   1.00447396e-02\n",
      "     -5.13983890e-03   1.32887274e-01   4.85842042e-02  -6.72378391e-02]]\n",
      "\n",
      "  [[ -1.19183294e-01  -5.92271276e-02   1.62569150e-01   7.46428221e-02\n",
      "      1.00773893e-01   6.83628488e-03  -4.05076966e-02  -3.36920880e-02\n",
      "     -5.07030729e-03   1.88107371e-01  -3.27211581e-02   6.42581135e-02\n",
      "     -1.00529864e-01   1.89863779e-02   5.72701655e-02   4.19637039e-02]]\n",
      "\n",
      "  [[  1.31058171e-01  -1.70062229e-01   7.47505948e-03   7.68562034e-02\n",
      "     -5.35969697e-02  -6.03664815e-02  -1.77419968e-02  -1.11041389e-01\n",
      "      9.91858318e-02   8.42565522e-02  -1.27541587e-01  -9.89523623e-03\n",
      "      1.06271289e-01   9.34201665e-03   1.31079732e-02  -5.73795214e-02]]\n",
      "\n",
      "  [[  1.39822006e-01   9.95060802e-02  -1.69622406e-01   1.98550150e-01\n",
      "     -2.81626489e-02   6.12680130e-02   3.02468572e-04  -3.56869772e-02\n",
      "     -3.43410410e-02  -1.66447908e-01   7.02388352e-03   2.13608798e-02\n",
      "     -1.73534099e-02   1.63151383e-01  -6.03256188e-02  -8.82124603e-02]]\n",
      "\n",
      "  [[ -4.64885905e-02   2.68658549e-02  -1.72437206e-02   2.17933487e-02\n",
      "     -3.66545543e-02   2.57907039e-03  -3.45176160e-02  -3.22183855e-02\n",
      "     -1.28997162e-01   1.10932402e-01  -1.72420461e-02  -1.74222663e-02\n",
      "      6.03114478e-02   5.85183017e-02   1.12466244e-02  -9.97565240e-02]]]\n",
      "\n",
      "\n",
      " [[[  7.49262720e-02   1.27833905e-02   3.14764492e-02   8.14879984e-02\n",
      "     -8.46642442e-03  -1.69692963e-01  -8.92239437e-02  -6.07579760e-02\n",
      "     -5.85360043e-02   4.53296117e-02  -8.09699856e-03  -2.16052160e-02\n",
      "      1.66215166e-01  -5.74711077e-02  -1.32345676e-01   1.16880968e-01]]\n",
      "\n",
      "  [[ -7.60892481e-02   6.52733669e-02   1.19766993e-02   3.65477689e-02\n",
      "     -2.64185965e-02   9.22991261e-02   8.06918368e-02   1.07218167e-02\n",
      "      1.14163280e-01   1.00205299e-02  -1.13784801e-02   4.51530097e-03\n",
      "     -5.79605587e-02   1.83806419e-02  -6.09416328e-02   1.85601816e-01]]\n",
      "\n",
      "  [[  1.42746463e-01  -5.16912006e-02  -8.99144262e-02   1.17833689e-01\n",
      "      5.26542626e-02   2.93725077e-02  -6.17130809e-02   2.59645525e-02\n",
      "      1.88292917e-02  -1.44766655e-03   1.95730608e-02   1.64208934e-01\n",
      "     -1.62865937e-01   6.52566999e-02  -9.76582915e-02   8.91869515e-02]]\n",
      "\n",
      "  [[ -1.63358852e-01  -2.20259069e-04   4.87115346e-02  -2.72878315e-02\n",
      "      1.20753907e-01   1.30106136e-01   1.08279482e-01   1.39516860e-01\n",
      "     -4.04458418e-02  -7.01328143e-02  -1.92893166e-02   9.77870449e-03\n",
      "     -1.16322540e-01   4.80385870e-02   7.83500727e-03   1.86164573e-01]]\n",
      "\n",
      "  [[  1.28485516e-01  -3.85718085e-02  -8.89828876e-02   1.96691021e-01\n",
      "      7.39832148e-02   1.70925017e-02   4.78736497e-02  -1.62989665e-02\n",
      "     -8.68737847e-02  -1.67716127e-02  -1.37535721e-01  -1.23960532e-01\n",
      "     -5.13165109e-02   4.67078835e-02  -2.04932243e-02   1.04658984e-01]]\n",
      "\n",
      "  [[ -1.16011694e-01  -2.23802011e-02  -6.05028644e-02   1.15831815e-01\n",
      "     -1.13757126e-01  -4.45826612e-02  -1.35120660e-01   9.08579454e-02\n",
      "     -3.12364530e-02   9.90431830e-02  -3.34960520e-02  -5.18589579e-02\n",
      "      4.17218842e-02  -1.26459584e-01  -1.23463131e-01  -1.38129026e-01]]]\n",
      "\n",
      "\n",
      " [[[  8.89753848e-02   7.71211926e-03   5.45672290e-02  -7.67178182e-03\n",
      "     -5.18665724e-02  -5.21900272e-03   5.88679016e-02   2.12329756e-02\n",
      "      1.49512470e-01   6.40088841e-02  -4.03492041e-02  -5.15692011e-02\n",
      "     -1.05024196e-01   5.25642894e-02   3.93200926e-02   4.20640372e-02]]\n",
      "\n",
      "  [[ -1.30027235e-01  -5.06792068e-02   2.31548934e-03  -8.76938552e-03\n",
      "     -1.09024599e-01   1.36883944e-01   5.99172851e-03   1.03749923e-01\n",
      "      5.02456166e-03  -1.76322088e-01   7.99081624e-02  -2.67105978e-02\n",
      "      4.98069935e-02   9.92556736e-02   6.08123243e-02  -7.53233507e-02]]\n",
      "\n",
      "  [[  9.59654525e-02   7.44460598e-02  -7.76058361e-02  -1.59689173e-01\n",
      "      3.98812741e-02   1.97570931e-04   2.22332519e-03   1.97735697e-01\n",
      "     -7.93028399e-02   9.09027979e-02   2.97980607e-02   5.23708127e-02\n",
      "     -9.61550698e-02  -1.79689869e-01   1.22259021e-01   8.95665661e-02]]\n",
      "\n",
      "  [[  7.58811682e-02   7.64123946e-02  -1.06084170e-02   1.31815393e-02\n",
      "      4.70490567e-02   6.24741754e-03  -7.81215057e-02   8.79650414e-02\n",
      "     -1.50938407e-01  -2.16920692e-02   2.20821016e-02   1.13940872e-01\n",
      "     -1.18692555e-01   6.42282814e-02   3.76890264e-02   1.24442697e-01]]\n",
      "\n",
      "  [[  6.80165663e-02   1.16964374e-02  -3.34259383e-02  -4.49876301e-02\n",
      "      9.92697403e-02   3.35429353e-03   3.96258347e-02   5.02462052e-02\n",
      "      1.63170859e-01  -5.17080128e-02  -2.61140671e-02   6.72912672e-02\n",
      "      8.90311822e-02  -1.54439598e-01   1.33205742e-01  -8.60428736e-02]]\n",
      "\n",
      "  [[  1.09639466e-01  -8.18041712e-02   9.41788312e-03   2.16065068e-02\n",
      "      6.19451068e-02  -3.14686000e-02  -8.55600759e-02   6.99865445e-02\n",
      "     -6.52749166e-02   8.41774195e-02  -1.66583642e-01   7.21704960e-02\n",
      "      1.83651038e-02  -5.07782772e-02   1.72097474e-01  -1.44849330e-01]]]\n",
      "\n",
      "\n",
      " [[[ -8.28628689e-02  -6.30729422e-02  -3.13712843e-02  -7.41569772e-02\n",
      "     -4.75068251e-03   5.39392419e-02  -1.81293953e-02   1.15986824e-01\n",
      "     -5.04761413e-02  -2.96885669e-02   1.25958202e-02  -6.35557845e-02\n",
      "     -3.94744501e-02  -5.18050753e-02   1.56973526e-01   1.40021130e-01]]\n",
      "\n",
      "  [[  1.31788207e-02  -1.50325194e-01   5.86040430e-02   5.24121942e-03\n",
      "     -9.58772674e-02   1.60116460e-02  -9.29943994e-02   4.96012866e-02\n",
      "     -7.08415136e-02   3.21485475e-02  -3.65686677e-02   7.29423687e-02\n",
      "      1.52404547e-01   7.27931708e-02   6.70051724e-02  -1.61694214e-01]]\n",
      "\n",
      "  [[  6.58902824e-02   6.71693981e-02   2.32880991e-02  -1.40590416e-02\n",
      "     -7.87641332e-02   1.56278417e-01  -1.21206500e-01  -2.60800309e-02\n",
      "     -1.12049811e-01  -1.21338800e-01  -6.29550442e-02   9.12821516e-02\n",
      "      2.50752061e-03   5.69522157e-02  -7.57399723e-02  -2.41341423e-02]]\n",
      "\n",
      "  [[ -1.54305417e-02   8.28038380e-02  -1.30235851e-01   3.72540653e-02\n",
      "     -1.51722014e-01  -1.07782058e-01   1.41069070e-01  -2.87811104e-02\n",
      "      1.62094161e-01   6.26718858e-03  -7.59768933e-02   1.45846680e-01\n",
      "      1.97310790e-01  -5.85501492e-02   3.29400375e-02   5.16208075e-02]]\n",
      "\n",
      "  [[ -8.63801390e-02  -4.57830541e-02   9.53367203e-02  -3.93643342e-02\n",
      "     -1.37652323e-01  -1.13596976e-01  -4.94286744e-03  -5.21160029e-02\n",
      "      7.96448216e-02   9.65861902e-02  -1.13252535e-01   6.02213331e-02\n",
      "      9.34474990e-02   6.72462806e-02  -2.28464929e-03   4.65522408e-02]]\n",
      "\n",
      "  [[ -1.62061956e-03   2.69619022e-02  -2.16657724e-02   4.72937673e-02\n",
      "     -1.02583960e-01   5.58997467e-02  -2.91375946e-02  -9.19005647e-02\n",
      "      9.20769647e-02  -1.55359238e-01  -1.31739900e-01  -2.87952758e-02\n",
      "     -5.60907200e-02  -7.76516721e-02   8.13511238e-02  -1.77727923e-01]]]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[[[[ -2.17122864e-02  -2.12721806e-02  -8.58871674e-04 ...,\n",
      "     -8.43241625e-03  -6.12541139e-02  -2.05916539e-02]\n",
      "   [  4.38069180e-02  -1.33890688e-01  -3.77395451e-02 ...,\n",
      "      1.77477688e-01  -3.83120263e-03   1.07661985e-01]\n",
      "   [ -8.86555314e-02   5.71676604e-02   7.48491986e-03 ...,\n",
      "     -1.22790866e-01  -1.46286681e-01   1.03043258e-01]\n",
      "   ..., \n",
      "   [ -1.00996308e-01  -1.15287960e-01  -5.42470403e-02 ...,\n",
      "     -7.76125863e-02   6.31841347e-02  -6.23710267e-02]\n",
      "   [ -6.06574379e-02  -3.61766927e-02  -4.73555829e-03 ...,\n",
      "     -7.10306168e-02   6.65422156e-03  -1.78909048e-01]\n",
      "   [ -1.70835927e-02  -3.20670009e-03  -9.84406397e-02 ...,\n",
      "      5.74964173e-02  -8.42706766e-03  -3.53658735e-03]]\n",
      "\n",
      "  [[ -6.12733364e-02  -3.88026051e-02  -8.32973570e-02 ...,\n",
      "      3.68059240e-02   1.04776599e-01   1.77388117e-02]\n",
      "   [  1.41233848e-02   2.36664377e-02   2.32389141e-02 ...,\n",
      "      4.96010948e-03  -3.51852402e-02  -2.37104628e-04]\n",
      "   [ -3.03641651e-02   3.81247252e-02   1.03736535e-01 ...,\n",
      "      1.37953371e-01   4.16558422e-02  -8.53887424e-02]\n",
      "   ..., \n",
      "   [ -6.74580857e-02   5.24570160e-02  -7.24997511e-03 ...,\n",
      "     -7.25088269e-02   2.21187603e-02   8.91585052e-02]\n",
      "   [ -7.51440972e-02   8.56716931e-02   1.09162010e-01 ...,\n",
      "     -3.77713442e-02  -7.12425038e-02   4.58478592e-02]\n",
      "   [  5.83136976e-02   2.20485753e-03  -8.68035480e-02 ...,\n",
      "     -8.67464170e-02   8.07643309e-02  -1.13206230e-01]]\n",
      "\n",
      "  [[  1.20595552e-01   2.37753261e-02   9.88137946e-02 ...,\n",
      "      1.39831826e-02  -1.04358522e-02   4.41499008e-03]\n",
      "   [  1.00141071e-01   6.67779073e-02  -2.34911945e-02 ...,\n",
      "     -1.81601048e-01  -4.86094616e-02  -1.05863653e-01]\n",
      "   [ -1.22176267e-01  -7.88779408e-02   4.54835817e-02 ...,\n",
      "     -4.88283625e-03  -1.45352647e-01   1.94989130e-01]\n",
      "   ..., \n",
      "   [ -7.82967918e-03   2.61916816e-02   6.15957566e-02 ...,\n",
      "     -8.14349949e-02  -2.46293435e-04   1.88143330e-03]\n",
      "   [  8.75801072e-02  -3.22856270e-02   8.84838626e-02 ...,\n",
      "      2.83576641e-02   9.82118323e-02   6.59645125e-02]\n",
      "   [  1.08955968e-02   1.48132965e-01   3.11868936e-02 ...,\n",
      "     -8.88916180e-02   1.67393610e-01   1.84609815e-01]]\n",
      "\n",
      "  [[ -8.17149207e-02  -6.71245977e-02   1.07426479e-01 ...,\n",
      "     -6.73335418e-02  -6.91026077e-02  -5.57583086e-02]\n",
      "   [  2.12237462e-02  -1.33931855e-04   1.33380726e-01 ...,\n",
      "     -1.22935213e-01  -1.42365713e-02   2.42680721e-02]\n",
      "   [ -1.18718162e-01  -4.04511914e-02  -1.11715253e-02 ...,\n",
      "      4.63987999e-02  -9.11166593e-02   1.88164860e-01]\n",
      "   ..., \n",
      "   [  6.35672435e-02  -3.66559066e-02  -9.38209742e-02 ...,\n",
      "      1.33963093e-01  -1.20197777e-02   4.92300354e-02]\n",
      "   [  9.30398628e-02  -1.62319526e-01   7.03450590e-02 ...,\n",
      "      1.25904471e-01  -6.59425110e-02   5.26091345e-02]\n",
      "   [ -6.66176900e-02   9.65029225e-02   4.24292423e-02 ...,\n",
      "     -5.72782271e-02   1.29511163e-01   1.84288755e-01]]\n",
      "\n",
      "  [[  1.17699996e-01   3.53518291e-03  -5.15623689e-02 ...,\n",
      "      8.99399668e-02  -5.00468090e-02   9.75377336e-02]\n",
      "   [  1.19902836e-02   1.78797450e-02   1.03946678e-01 ...,\n",
      "      3.19585428e-02   4.10216860e-02   1.66518893e-02]\n",
      "   [ -4.65045236e-02   3.19025144e-02  -1.24291196e-01 ...,\n",
      "      2.78668609e-02   8.15731511e-02  -1.68724939e-01]\n",
      "   ..., \n",
      "   [  7.20347315e-02   1.58385023e-01   1.70033351e-01 ...,\n",
      "      1.89528670e-02   7.94759765e-03   4.14304361e-02]\n",
      "   [  1.43519565e-01   1.36365995e-01   2.79411878e-02 ...,\n",
      "     -5.86367957e-02   2.26837117e-02  -4.03312929e-02]\n",
      "   [  1.20153390e-01  -1.03365175e-01   5.44443317e-02 ...,\n",
      "      4.73376326e-02  -3.99297513e-02  -1.21453084e-01]]\n",
      "\n",
      "  [[  1.34970590e-01  -1.04529239e-01  -2.07973551e-02 ...,\n",
      "     -6.67478815e-02  -1.99349627e-01   3.41357850e-02]\n",
      "   [  3.08533050e-02  -9.66471806e-02  -5.87371960e-02 ...,\n",
      "     -3.79779637e-02   8.49761069e-02  -1.61287934e-02]\n",
      "   [  8.19745474e-03  -2.67484318e-02  -4.01298515e-02 ...,\n",
      "     -7.85715207e-02  -2.15230845e-02   1.46111712e-01]\n",
      "   ..., \n",
      "   [ -2.79144533e-02   1.00023225e-01   6.87748045e-02 ...,\n",
      "      9.27664861e-02  -6.00048192e-02   4.63755168e-02]\n",
      "   [  6.03579953e-02   5.09678312e-02  -1.45399749e-01 ...,\n",
      "     -6.60044923e-02   1.76802967e-02   3.21495384e-02]\n",
      "   [ -1.57020882e-01  -1.87606625e-02  -2.91338116e-02 ...,\n",
      "      6.65991276e-04   1.04610637e-01   1.64042085e-01]]]\n",
      "\n",
      "\n",
      " [[[  8.98978338e-02  -5.36968708e-02  -9.74672809e-02 ...,\n",
      "      2.62583476e-02  -7.75502846e-02  -7.38977939e-02]\n",
      "   [ -4.02623899e-02  -1.41928166e-01   1.84704419e-02 ...,\n",
      "      3.90581116e-02  -7.40626454e-02   3.38133015e-02]\n",
      "   [ -1.54608330e-02   1.13870814e-01   5.96568361e-02 ...,\n",
      "     -1.96337968e-01   1.89103767e-01  -2.14067772e-02]\n",
      "   ..., \n",
      "   [ -8.09571296e-02   3.58246230e-02   2.97279712e-02 ...,\n",
      "     -1.15232263e-02  -9.71691310e-02  -1.32318228e-01]\n",
      "   [  1.45004809e-01   6.64102435e-02   2.33449023e-02 ...,\n",
      "      6.94215447e-02  -9.57913548e-02   1.72414556e-01]\n",
      "   [  8.63884166e-02  -1.23071950e-02   8.64374265e-02 ...,\n",
      "      1.02400057e-01   1.52130872e-02  -3.54051143e-02]]\n",
      "\n",
      "  [[ -2.36436166e-02  -2.69323178e-02   6.88391775e-02 ...,\n",
      "     -1.57795951e-01   1.66064158e-01  -8.18125680e-02]\n",
      "   [  2.10264488e-03  -9.65358019e-02   2.43580993e-02 ...,\n",
      "     -1.42455280e-01   7.94301182e-03   3.55291925e-03]\n",
      "   [ -1.39937013e-01   3.14103328e-02  -1.14098601e-02 ...,\n",
      "      8.94776285e-02   2.24871023e-04   6.23253956e-02]\n",
      "   ..., \n",
      "   [  1.59990102e-01  -7.35132843e-02  -3.87406424e-02 ...,\n",
      "      1.79317556e-02   7.15793297e-03   2.48667654e-02]\n",
      "   [  2.22338457e-02  -1.69384144e-02   1.35008454e-01 ...,\n",
      "      1.60158515e-01   1.57070365e-02  -2.88944375e-02]\n",
      "   [  1.64357454e-01   7.19108731e-02   6.21630251e-02 ...,\n",
      "      6.69635683e-02  -1.27099395e-01   1.02862857e-01]]\n",
      "\n",
      "  [[  1.82803776e-02  -6.06839135e-02  -6.67917579e-02 ...,\n",
      "     -4.33435403e-02  -2.74131987e-02   1.67808250e-01]\n",
      "   [  3.56927663e-02  -9.38091706e-03  -6.90422952e-02 ...,\n",
      "     -2.02604253e-02   1.24420896e-01  -8.85419026e-02]\n",
      "   [  1.60953905e-02   1.55328605e-02  -1.77575544e-01 ...,\n",
      "      1.70544937e-01  -2.24805251e-02   1.71333477e-01]\n",
      "   ..., \n",
      "   [  7.15591311e-02   7.87802562e-02  -2.33558975e-02 ...,\n",
      "     -2.29456313e-02  -6.08129501e-02   1.23906434e-02]\n",
      "   [ -5.14170714e-02   3.05014160e-02  -1.54486865e-01 ...,\n",
      "      1.52945280e-01  -2.09769774e-02   2.49643307e-02]\n",
      "   [  8.63541290e-02   1.05044760e-01   9.44865793e-02 ...,\n",
      "     -7.54117891e-02  -1.95453331e-01  -7.81054795e-02]]\n",
      "\n",
      "  [[ -2.97138635e-02   7.37224519e-02   1.19672298e-01 ...,\n",
      "      5.63881062e-02  -9.43255052e-02  -4.72697057e-02]\n",
      "   [ -9.74682253e-03  -1.09940745e-01  -1.05701322e-02 ...,\n",
      "     -9.51724201e-02  -7.69288093e-02   9.54474732e-02]\n",
      "   [ -5.92475422e-02  -6.22973442e-02  -7.05994293e-02 ...,\n",
      "      7.05179125e-02  -8.33431333e-02  -1.30924195e-01]\n",
      "   ..., \n",
      "   [ -1.67352427e-02   4.88313213e-02  -3.15557234e-02 ...,\n",
      "      1.34877667e-01  -4.60011028e-02  -3.92125249e-02]\n",
      "   [  2.26227771e-02  -3.38922031e-02   1.51005968e-01 ...,\n",
      "      2.72093993e-02  -9.53661278e-02   3.29981148e-02]\n",
      "   [ -3.53857316e-02  -9.91642624e-02  -7.10201040e-02 ...,\n",
      "     -7.53971413e-02  -6.21311031e-02   1.36447772e-01]]\n",
      "\n",
      "  [[ -3.65770645e-02  -1.11367799e-01  -1.06532387e-02 ...,\n",
      "      1.34426937e-01   1.19834997e-01  -5.45237772e-02]\n",
      "   [ -1.60201088e-01   1.80502295e-01  -9.87331495e-02 ...,\n",
      "     -1.38462231e-01  -1.96981281e-01  -1.13402903e-01]\n",
      "   [  1.40136167e-01   1.01074420e-01  -8.70680995e-03 ...,\n",
      "     -4.99160290e-02   4.97212224e-02   4.81374934e-02]\n",
      "   ..., \n",
      "   [  1.63913965e-01   8.17581043e-02  -1.20071806e-01 ...,\n",
      "      1.40914321e-02   1.49077773e-01  -5.51366173e-02]\n",
      "   [ -5.66911064e-02   1.63571477e-01  -1.73881810e-04 ...,\n",
      "      1.05140181e-02  -8.87355432e-02   3.15038748e-02]\n",
      "   [  4.25922126e-02  -3.17013226e-02  -1.08109429e-01 ...,\n",
      "     -4.65564691e-02  -5.62874265e-02  -1.32191973e-02]]\n",
      "\n",
      "  [[  7.56295770e-02  -8.79245251e-02   6.31298646e-02 ...,\n",
      "     -9.50655248e-03  -1.00383900e-01   1.56844541e-01]\n",
      "   [  1.82834700e-01  -6.20613173e-02  -3.91480215e-02 ...,\n",
      "      1.22408681e-01   4.31734063e-02  -8.13856870e-02]\n",
      "   [ -1.24587175e-02   3.34372371e-02  -8.05967078e-02 ...,\n",
      "      1.16023950e-01  -5.09068258e-02  -1.33016929e-01]\n",
      "   ..., \n",
      "   [ -3.21561731e-02  -1.13297224e-01  -1.12754833e-02 ...,\n",
      "     -1.03597365e-01   2.31300704e-02  -8.69994909e-02]\n",
      "   [  5.06997183e-02   5.58564849e-02   1.16663456e-01 ...,\n",
      "     -1.55625999e-01   7.19695389e-02  -6.92177787e-02]\n",
      "   [  2.92612668e-02   1.75596271e-02   9.18440446e-02 ...,\n",
      "      1.02606393e-01  -3.83109935e-02  -6.23109117e-02]]]\n",
      "\n",
      "\n",
      " [[[ -1.60300791e-01   1.79279000e-02  -5.27492724e-02 ...,\n",
      "     -7.76427006e-03   4.32897173e-02   6.27958104e-02]\n",
      "   [  9.73239467e-02   1.05072074e-01  -3.82302254e-02 ...,\n",
      "     -1.11947052e-01  -2.65980065e-02  -4.16103937e-02]\n",
      "   [ -2.99399383e-02  -9.82696339e-02  -3.70293669e-02 ...,\n",
      "      7.09686875e-02   1.01595700e-01   1.00163214e-01]\n",
      "   ..., \n",
      "   [ -8.32424499e-03   5.28661683e-02   2.94365939e-02 ...,\n",
      "      1.04434289e-01   7.41901249e-02   1.51023239e-01]\n",
      "   [  6.51023258e-03  -1.05807863e-01   8.56443401e-03 ...,\n",
      "     -9.34852287e-02   5.36630861e-02  -4.33669286e-03]\n",
      "   [  2.85989922e-02   1.20033495e-01  -4.36605774e-02 ...,\n",
      "      3.89030688e-02   4.01091985e-02   9.86058787e-02]]\n",
      "\n",
      "  [[  7.06781372e-02   2.30323151e-02  -6.04715757e-02 ...,\n",
      "      4.68109213e-02  -6.92491606e-02  -1.33245289e-01]\n",
      "   [ -1.27853021e-01  -7.85623863e-02   1.04413055e-01 ...,\n",
      "      1.42864347e-01   9.23416689e-02  -8.18883106e-02]\n",
      "   [ -6.60447031e-02   2.94931531e-02   1.13464169e-01 ...,\n",
      "     -1.76111788e-01   4.20714989e-02   1.78743511e-01]\n",
      "   ..., \n",
      "   [  1.24893047e-01   2.83523109e-02   1.55091032e-01 ...,\n",
      "     -1.11470126e-01   6.69202954e-02   1.60932183e-01]\n",
      "   [  1.64738864e-01  -5.50529025e-02  -6.69024438e-02 ...,\n",
      "      3.86876501e-02  -1.01575591e-01   3.54496390e-02]\n",
      "   [  9.61632058e-02   1.32843137e-01   9.43027716e-03 ...,\n",
      "     -3.69414613e-02   1.44658789e-01   3.58397476e-02]]\n",
      "\n",
      "  [[  3.76916602e-02  -4.88085113e-02   5.32023124e-02 ...,\n",
      "      1.38592556e-01  -1.65624842e-01   6.69812486e-02]\n",
      "   [  4.75078002e-02   1.61646437e-02   6.10697754e-02 ...,\n",
      "     -2.48529129e-02   7.10286796e-02  -1.19269900e-01]\n",
      "   [ -3.27176042e-02  -1.15862601e-01   8.00643712e-02 ...,\n",
      "     -8.35794732e-02   5.85299022e-02   1.61377937e-01]\n",
      "   ..., \n",
      "   [ -4.09233347e-02  -5.51123582e-02  -5.09977825e-02 ...,\n",
      "     -1.05924290e-02   1.34820892e-02   5.05396239e-02]\n",
      "   [  8.75508636e-02   7.22277984e-02  -8.74250755e-02 ...,\n",
      "      7.23675219e-03   2.51020342e-02  -2.74505354e-02]\n",
      "   [ -1.19586721e-01   6.31748363e-02   1.59662310e-02 ...,\n",
      "      1.14927866e-01   8.26797411e-02  -1.20012477e-01]]\n",
      "\n",
      "  [[ -1.22877538e-01  -1.32968754e-01  -9.14955288e-02 ...,\n",
      "     -1.16080754e-01  -8.20040181e-02   1.17976097e-02]\n",
      "   [ -2.94724684e-02  -5.49643708e-04   8.12789798e-02 ...,\n",
      "      5.17839845e-03  -1.61179647e-01   1.32017761e-01]\n",
      "   [ -6.41523749e-02   1.60352349e-01   1.21188879e-01 ...,\n",
      "      2.00267900e-02   1.27604797e-01  -2.71403994e-02]\n",
      "   ..., \n",
      "   [ -9.61067900e-02   1.15725361e-02  -3.22922133e-02 ...,\n",
      "      1.32271601e-02   3.96223692e-03  -8.77607465e-02]\n",
      "   [ -5.51347397e-02  -1.10351801e-01   1.14922477e-02 ...,\n",
      "     -5.47085814e-02   8.46690983e-02   9.07461122e-02]\n",
      "   [ -3.35657224e-02   6.24920502e-02  -1.67175621e-01 ...,\n",
      "      1.85440645e-01   4.26434949e-02   1.04313334e-02]]\n",
      "\n",
      "  [[ -1.37586400e-01  -1.59609374e-02  -3.34679037e-02 ...,\n",
      "      1.32084042e-01   1.17949389e-01  -2.49888133e-02]\n",
      "   [  1.06955849e-01   2.01898441e-02  -1.86352488e-02 ...,\n",
      "      1.06161665e-02  -1.11555390e-01   2.08774433e-02]\n",
      "   [  5.30062206e-02   8.93634781e-02  -8.40972662e-02 ...,\n",
      "      9.16977748e-02  -2.25354154e-02  -3.26054431e-02]\n",
      "   ..., \n",
      "   [ -1.51900738e-01  -4.62862663e-02  -7.68022612e-03 ...,\n",
      "     -3.31399888e-02  -9.40966606e-03  -9.25315544e-03]\n",
      "   [  2.37341281e-02   1.68392509e-01  -4.52434048e-02 ...,\n",
      "     -2.10252758e-02  -1.34558290e-01   6.84426278e-02]\n",
      "   [  1.44939020e-01   1.27949063e-02  -4.68418328e-03 ...,\n",
      "     -6.57349005e-02  -9.13762525e-02   3.12114190e-02]]\n",
      "\n",
      "  [[  8.76974612e-02  -1.67476624e-01  -1.34876922e-01 ...,\n",
      "     -6.81275874e-02  -1.04019046e-02  -7.23742619e-02]\n",
      "   [ -1.55018987e-02   1.21847175e-01  -9.61346850e-02 ...,\n",
      "     -3.32725681e-02   1.85080528e-01  -1.96009412e-01]\n",
      "   [  9.22606587e-02  -2.98959259e-02   8.99872854e-02 ...,\n",
      "     -1.30865514e-01  -1.09385133e-01   1.13181889e-01]\n",
      "   ..., \n",
      "   [  1.01153947e-01   6.33353367e-02   5.03965393e-02 ...,\n",
      "      1.69823524e-02   7.34996870e-02   5.33500426e-02]\n",
      "   [  4.20846604e-02   8.27213004e-02  -1.32054389e-02 ...,\n",
      "     -1.82837710e-01  -8.36527254e-03   1.32324994e-02]\n",
      "   [  3.94912176e-02   3.33272181e-02  -1.42066762e-01 ...,\n",
      "      4.86636646e-02   3.89581174e-02  -1.16515696e-01]]]\n",
      "\n",
      "\n",
      " [[[ -9.93325096e-03  -5.59345745e-02  -4.73426376e-03 ...,\n",
      "     -8.88634175e-02   2.46757660e-02   1.35129079e-01]\n",
      "   [ -7.92211816e-02   1.20249785e-01   5.79481423e-02 ...,\n",
      "     -2.41709519e-02  -1.30000457e-01  -2.63894144e-02]\n",
      "   [  3.86086814e-02  -6.91538630e-03  -3.20596211e-02 ...,\n",
      "     -1.04032040e-01   1.13497786e-01   9.02766138e-02]\n",
      "   ..., \n",
      "   [  9.07967389e-02  -7.96744376e-02   1.18485000e-02 ...,\n",
      "     -6.21040240e-02  -2.98159965e-03   7.57278427e-02]\n",
      "   [ -8.59912559e-02  -5.97879291e-02   3.51544395e-02 ...,\n",
      "     -5.42287938e-02   2.11920924e-02  -3.65657173e-02]\n",
      "   [ -3.49963233e-02   8.72883350e-02  -7.66960084e-02 ...,\n",
      "     -1.86534569e-01   7.65672550e-02   2.16468628e-02]]\n",
      "\n",
      "  [[  4.03175354e-02   7.27200229e-03  -1.18525559e-02 ...,\n",
      "     -2.83898804e-02   9.14566666e-02  -1.19125471e-02]\n",
      "   [  2.10706405e-02   6.65532351e-02   1.23326115e-01 ...,\n",
      "     -3.12905088e-02  -5.56610525e-03   3.10907420e-02]\n",
      "   [  7.09945038e-02  -1.36833414e-01  -1.62825584e-02 ...,\n",
      "     -1.03739738e-01   5.35570160e-02   2.53216978e-02]\n",
      "   ..., \n",
      "   [  5.98933212e-02  -6.23081950e-03   6.48674890e-02 ...,\n",
      "     -5.13169952e-02  -2.02987120e-02   1.72454163e-01]\n",
      "   [ -6.56332076e-02  -6.48836717e-02   1.77849889e-01 ...,\n",
      "     -1.47905825e-02   1.14741556e-01  -5.15302531e-02]\n",
      "   [ -4.52716788e-03  -5.25050350e-02   8.62114429e-02 ...,\n",
      "     -2.25179028e-02  -1.23371355e-01   2.68763136e-02]]\n",
      "\n",
      "  [[ -1.63044389e-02  -1.02697387e-01  -1.03493733e-02 ...,\n",
      "     -1.73615545e-01   1.98548958e-01  -9.41857919e-02]\n",
      "   [  5.76897375e-02  -1.13580108e-01  -5.04310839e-02 ...,\n",
      "      6.03156798e-02  -5.53705469e-02   3.61984931e-02]\n",
      "   [ -2.51395311e-02  -4.19415608e-02   7.68748298e-02 ...,\n",
      "      7.36588016e-02   3.32837664e-02  -1.85099721e-01]\n",
      "   ..., \n",
      "   [ -5.29917777e-02   9.09256469e-03   7.42778182e-02 ...,\n",
      "     -4.39444743e-02   6.34871200e-02  -3.19144204e-02]\n",
      "   [  9.66660306e-02  -9.55764428e-02   2.96887401e-02 ...,\n",
      "      1.73058324e-02   1.15453057e-01   1.51416346e-01]\n",
      "   [  9.17383954e-02   7.09165856e-02   1.46238074e-01 ...,\n",
      "     -2.26862859e-02   6.22002855e-02  -1.13514952e-01]]\n",
      "\n",
      "  [[  2.42611617e-02   7.43226036e-02  -1.15904413e-01 ...,\n",
      "      5.15790097e-02  -1.91363171e-02   1.56343743e-01]\n",
      "   [  4.23781984e-02   3.83920185e-02  -1.69854611e-02 ...,\n",
      "      1.47120938e-01   4.51388471e-02   5.32419086e-02]\n",
      "   [ -3.02129309e-03   2.23108213e-02  -2.73612775e-02 ...,\n",
      "      2.26998832e-02   4.07265201e-02   1.15297869e-01]\n",
      "   ..., \n",
      "   [  2.74361856e-03   7.70599544e-02   3.41863856e-02 ...,\n",
      "      4.36489703e-03   1.65538818e-01   6.23512045e-02]\n",
      "   [ -6.89891428e-02  -2.08315235e-02   1.08335093e-01 ...,\n",
      "      4.32720631e-02  -5.36797158e-02   6.20079925e-03]\n",
      "   [ -1.14329897e-01  -3.15846838e-02  -6.24816865e-03 ...,\n",
      "      7.24430829e-02  -2.29806025e-02   3.58646028e-02]]\n",
      "\n",
      "  [[ -1.31516522e-02  -4.88945208e-02   5.05800061e-02 ...,\n",
      "     -2.98067164e-02  -3.21146250e-02   2.61916798e-02]\n",
      "   [  9.07815322e-02  -1.37650549e-01   1.37740716e-01 ...,\n",
      "     -1.42187893e-01  -1.13583766e-01   8.30490049e-03]\n",
      "   [  7.41646513e-02   7.33809844e-02   1.28232660e-02 ...,\n",
      "     -4.22534421e-02  -3.01131885e-03  -1.44857317e-01]\n",
      "   ..., \n",
      "   [ -8.44926238e-02  -7.78707862e-03   7.10532889e-02 ...,\n",
      "     -1.24643005e-01   1.02521278e-01   1.22992016e-01]\n",
      "   [  2.96728257e-02   1.92512129e-03   5.76340333e-02 ...,\n",
      "     -7.39792213e-02   1.43210758e-02   6.75223917e-02]\n",
      "   [ -1.59007162e-02   7.10865408e-02   1.45444050e-01 ...,\n",
      "     -7.20419586e-02   6.84696361e-02  -7.61677474e-02]]\n",
      "\n",
      "  [[ -6.59835115e-02   6.94070831e-02  -3.29545848e-02 ...,\n",
      "      3.30795906e-02  -1.15905501e-01  -2.00702390e-03]\n",
      "   [ -2.53337268e-02  -1.01016425e-02   6.11255430e-02 ...,\n",
      "      9.64787137e-03   1.02272905e-01   6.07722625e-03]\n",
      "   [ -1.10825084e-01   6.97239488e-02   5.93865179e-02 ...,\n",
      "     -8.57176259e-02  -4.78324406e-02   5.20319454e-02]\n",
      "   ..., \n",
      "   [ -1.94758940e-02   1.16233662e-01   7.69706294e-02 ...,\n",
      "     -1.76551834e-01  -1.62377223e-01   4.50596921e-02]\n",
      "   [ -8.06663856e-02  -5.76179512e-02   3.20295780e-03 ...,\n",
      "     -1.15940645e-01   1.75668612e-01   1.48951411e-01]\n",
      "   [ -1.37092233e-01   3.03025432e-02   3.29745896e-02 ...,\n",
      "     -6.95399791e-02   1.37899369e-02  -7.51594007e-02]]]\n",
      "\n",
      "\n",
      " [[[ -1.14906944e-01   3.91424932e-02   8.46414194e-02 ...,\n",
      "     -1.50248902e-02   1.41948136e-02  -4.80415002e-02]\n",
      "   [ -1.95871964e-01  -9.07012746e-02   7.78124481e-02 ...,\n",
      "     -9.28434655e-02   1.14574194e-01   3.62795629e-02]\n",
      "   [  5.98962605e-02  -4.26302850e-02   1.65052518e-01 ...,\n",
      "     -1.16778754e-01  -1.55454397e-03  -1.43962637e-01]\n",
      "   ..., \n",
      "   [  8.25778302e-03   3.26913707e-02  -1.87034294e-01 ...,\n",
      "     -1.21578062e-02  -2.40857364e-03  -1.60649419e-01]\n",
      "   [  2.30947025e-02  -4.83078212e-02  -7.92343635e-03 ...,\n",
      "     -6.58914000e-02   4.61858809e-02  -1.00213541e-02]\n",
      "   [ -1.48120765e-02   5.47335446e-02  -5.22408783e-02 ...,\n",
      "     -4.70203385e-02   1.18620329e-01  -1.50722235e-01]]\n",
      "\n",
      "  [[  3.55532430e-02  -1.49305210e-01  -9.63713899e-02 ...,\n",
      "      1.46830445e-02   1.25789091e-01   1.55606613e-01]\n",
      "   [  8.61931220e-02  -7.90020153e-02   9.57557037e-02 ...,\n",
      "      1.04528405e-01   1.28347769e-01  -7.95678571e-02]\n",
      "   [ -1.92579091e-01  -8.76731947e-02  -1.85610861e-01 ...,\n",
      "      1.02658980e-01  -3.73879522e-02   5.20592406e-02]\n",
      "   ..., \n",
      "   [ -1.64511502e-02  -7.49962702e-02   1.21407688e-01 ...,\n",
      "     -1.58836991e-01  -4.76983078e-02  -5.54258302e-02]\n",
      "   [  8.25574622e-02  -1.35830760e-01   3.29391621e-02 ...,\n",
      "     -1.32946922e-02   4.50260304e-02  -1.52473137e-01]\n",
      "   [ -1.08489357e-01  -7.30371624e-02   8.26223753e-03 ...,\n",
      "      1.19368602e-02   1.72840372e-01  -8.44009742e-02]]\n",
      "\n",
      "  [[ -1.31396158e-02   9.84536260e-02   7.87258968e-02 ...,\n",
      "      3.36575732e-02  -7.26416856e-02  -1.07683256e-01]\n",
      "   [ -5.92212379e-02   1.05692390e-02   3.83707322e-02 ...,\n",
      "     -7.80269355e-02  -1.82890892e-01  -6.75694570e-02]\n",
      "   [ -2.76436992e-02  -7.14757061e-03   1.56031074e-02 ...,\n",
      "     -1.18066028e-01  -8.39882270e-02  -3.37741040e-02]\n",
      "   ..., \n",
      "   [  2.69174017e-02   2.21867990e-02   4.06608172e-02 ...,\n",
      "     -9.52121839e-02   6.88845515e-02  -6.86186180e-02]\n",
      "   [  1.41562102e-02   3.13547254e-02  -1.00994766e-01 ...,\n",
      "      6.90928027e-02   2.61785295e-02   4.57821749e-02]\n",
      "   [  2.31979461e-03   2.84778979e-02   5.03914140e-04 ...,\n",
      "     -3.89201636e-03  -3.78122069e-02  -2.12475471e-02]]\n",
      "\n",
      "  [[  3.02445441e-02   1.38364822e-01  -1.01144098e-01 ...,\n",
      "     -9.93940607e-02   6.30004406e-02  -1.33722518e-02]\n",
      "   [ -3.23684625e-02  -6.38033301e-02  -1.03827015e-01 ...,\n",
      "     -1.12298265e-01   9.03382339e-03  -1.83727503e-01]\n",
      "   [ -5.84437735e-02  -8.76081437e-02  -5.14534675e-02 ...,\n",
      "     -3.85852829e-02  -1.39093697e-01   3.10299341e-02]\n",
      "   ..., \n",
      "   [  7.78261349e-02   2.30571572e-02   2.71054003e-02 ...,\n",
      "     -4.29384373e-02  -4.84193340e-02  -7.55077153e-02]\n",
      "   [ -3.26292291e-02  -8.33257586e-02   1.23445047e-02 ...,\n",
      "      3.36623453e-02  -1.32868186e-01   5.97159676e-02]\n",
      "   [ -4.50497493e-03   6.56160936e-02  -1.37142837e-01 ...,\n",
      "      1.43538922e-01   3.27574722e-02   1.52726516e-01]]\n",
      "\n",
      "  [[ -7.13296309e-02  -9.91060063e-02  -7.85561427e-02 ...,\n",
      "      1.04695857e-01   1.98326930e-01  -4.33003381e-02]\n",
      "   [ -3.13405804e-02   1.38385728e-01   1.30716398e-01 ...,\n",
      "      4.23774645e-02   7.77638629e-02  -2.09697634e-02]\n",
      "   [  1.03433482e-01  -8.41622055e-02  -1.40312836e-01 ...,\n",
      "      1.99086323e-01  -1.29837617e-01  -1.39949307e-01]\n",
      "   ..., \n",
      "   [ -2.07421985e-02  -6.08306043e-02  -5.15867062e-02 ...,\n",
      "      4.58019301e-02   1.21724168e-02  -7.86145553e-02]\n",
      "   [ -7.63360932e-02   1.99805617e-01  -1.56528261e-02 ...,\n",
      "     -2.22047735e-02  -1.09930746e-02   7.18108788e-02]\n",
      "   [  1.64233632e-02  -1.51712477e-01   1.41885981e-01 ...,\n",
      "     -9.81528834e-02   7.26438910e-02  -2.27194857e-02]]\n",
      "\n",
      "  [[ -8.68972763e-02   1.18840419e-01   6.40904829e-02 ...,\n",
      "      8.22073147e-02   4.71505225e-02   7.61546940e-02]\n",
      "   [  1.01264417e-01  -2.44610328e-02   3.05730011e-02 ...,\n",
      "     -1.14573769e-01   8.38612095e-02   9.64025781e-02]\n",
      "   [ -8.38288739e-02   8.90504047e-02   1.34018093e-01 ...,\n",
      "     -5.99288158e-02   1.62483633e-01  -1.06495313e-01]\n",
      "   ..., \n",
      "   [  4.89985459e-02  -1.58711299e-02  -6.11026175e-02 ...,\n",
      "      9.45139006e-02   1.64628282e-01   1.13143303e-01]\n",
      "   [  1.67615995e-01  -1.83141406e-03  -5.16273938e-02 ...,\n",
      "     -2.23714188e-02  -7.33506009e-02   9.79960058e-03]\n",
      "   [ -7.75271729e-02   4.31306772e-02   3.80630083e-02 ...,\n",
      "      3.69271934e-02   3.47713046e-02  -8.44758376e-02]]]\n",
      "\n",
      "\n",
      " [[[ -5.26937209e-02   1.10313497e-01  -1.07541122e-01 ...,\n",
      "      9.10238456e-03   7.20675066e-02   1.32738175e-02]\n",
      "   [  2.82672234e-02   1.61054015e-01   5.95714934e-02 ...,\n",
      "     -7.93885514e-02  -4.22294298e-03   1.25757772e-02]\n",
      "   [  5.97732328e-02  -7.62628391e-02  -4.12958534e-03 ...,\n",
      "      1.71793904e-02   4.84427577e-03  -1.54626817e-01]\n",
      "   ..., \n",
      "   [  4.26448584e-02  -3.55524500e-03  -1.54816106e-01 ...,\n",
      "     -1.07963227e-01   1.21266823e-02  -8.48659277e-02]\n",
      "   [ -1.16074597e-02   1.60259008e-01   2.91525163e-02 ...,\n",
      "      4.14823443e-02  -5.05370274e-02  -1.89843029e-02]\n",
      "   [  1.84933096e-01   1.56453311e-01  -7.35472515e-02 ...,\n",
      "      7.29341209e-02  -6.56256750e-02   1.60654739e-01]]\n",
      "\n",
      "  [[  4.92660664e-02  -1.49558606e-02   3.37930284e-02 ...,\n",
      "      6.60952032e-02   3.73612829e-02  -1.71236303e-02]\n",
      "   [  3.24288197e-02  -1.89776674e-01  -8.01964998e-02 ...,\n",
      "      1.93925604e-01  -1.43057033e-01  -1.91531447e-03]\n",
      "   [ -1.84703067e-01  -1.43324304e-03   1.02569513e-01 ...,\n",
      "     -1.40757831e-02   3.99215110e-02  -4.98055890e-02]\n",
      "   ..., \n",
      "   [  1.43212989e-01  -7.41675943e-02  -2.57476280e-03 ...,\n",
      "      7.02702329e-02   1.62032142e-01   1.33467361e-01]\n",
      "   [  1.03493385e-01  -9.23357978e-02   1.16868094e-01 ...,\n",
      "      3.70638669e-02  -1.45640627e-01   6.96296319e-02]\n",
      "   [  4.77574486e-03  -1.06871925e-01   2.12233160e-02 ...,\n",
      "     -1.01790475e-02  -3.93651910e-02  -6.57227337e-02]]\n",
      "\n",
      "  [[ -1.50576949e-01  -4.42260765e-02  -1.21642068e-01 ...,\n",
      "      1.64781790e-02   2.74111666e-02  -2.90108677e-02]\n",
      "   [  8.54328554e-03  -1.03214225e-02  -1.16444333e-02 ...,\n",
      "     -1.10734023e-01   2.24760771e-02   1.35638282e-01]\n",
      "   [  1.51070371e-01   1.05549231e-01   7.32170194e-02 ...,\n",
      "      1.48592561e-01  -2.15711500e-02   5.02848700e-02]\n",
      "   ..., \n",
      "   [  1.64356649e-01  -5.78301623e-02   8.24390650e-02 ...,\n",
      "      7.52816722e-02   2.05639582e-02   9.97938775e-03]\n",
      "   [  4.57572984e-03   8.88534859e-02  -7.33051151e-02 ...,\n",
      "     -3.98218669e-02   9.73389223e-02  -9.87894684e-02]\n",
      "   [ -3.79055254e-02   1.50424972e-01   1.87794521e-01 ...,\n",
      "     -4.61415714e-03   1.15031362e-01   9.18368809e-03]]\n",
      "\n",
      "  [[  1.07140772e-01   5.63853979e-02   3.46720330e-02 ...,\n",
      "     -7.39214569e-02   2.26166062e-02   2.44443547e-02]\n",
      "   [  3.32491212e-02   7.82232732e-02   6.28179163e-02 ...,\n",
      "     -3.47812288e-02   1.38333708e-01  -1.05761312e-01]\n",
      "   [  5.74511662e-02   1.17840700e-01  -2.37526316e-02 ...,\n",
      "      3.78289148e-02  -4.46990542e-02  -1.82270762e-02]\n",
      "   ..., \n",
      "   [  6.50746096e-03   1.18207611e-01   1.04822338e-01 ...,\n",
      "     -9.19887647e-02   3.86137851e-02   1.24642095e-02]\n",
      "   [  1.25038594e-01  -9.47061740e-03  -9.73906443e-02 ...,\n",
      "     -1.78352650e-02   2.32539177e-02   1.31259620e-01]\n",
      "   [ -1.14297584e-01  -7.40954950e-02  -4.52654175e-02 ...,\n",
      "      7.27847666e-02  -2.95395553e-02  -8.32308382e-02]]\n",
      "\n",
      "  [[ -1.09057464e-01   1.39916688e-02   1.11434937e-01 ...,\n",
      "     -4.90734493e-03   8.01346153e-02   8.02218243e-02]\n",
      "   [ -2.18259003e-02   1.27493352e-01  -3.06860488e-02 ...,\n",
      "     -7.61044323e-02   4.01347280e-02   7.03218281e-02]\n",
      "   [  1.23920001e-01   4.68244180e-02   2.47018319e-02 ...,\n",
      "     -5.81225334e-03   2.29713749e-02  -6.96151629e-02]\n",
      "   ..., \n",
      "   [  1.13764562e-01  -1.42219663e-01  -1.43713355e-01 ...,\n",
      "      5.22326306e-02  -2.93691196e-02   1.26817850e-02]\n",
      "   [  8.15367233e-03  -1.46096870e-01  -1.27338514e-01 ...,\n",
      "      6.13174541e-03  -3.49113159e-02   4.52312529e-02]\n",
      "   [  1.75156072e-01   1.11480892e-01  -1.26452362e-02 ...,\n",
      "     -4.72023301e-02  -4.60137092e-02   1.66002318e-01]]\n",
      "\n",
      "  [[ -1.54294744e-01   1.34334313e-02  -8.05563331e-02 ...,\n",
      "     -3.97152118e-02  -1.29612043e-01   1.74089357e-01]\n",
      "   [ -9.66018140e-02  -1.12511786e-02   6.06827848e-02 ...,\n",
      "     -6.32631257e-02   2.10324954e-02  -2.23016436e-03]\n",
      "   [  2.84343418e-02   1.70922682e-01  -9.51111689e-02 ...,\n",
      "     -3.80686857e-02   1.08548412e-02   1.47427917e-01]\n",
      "   ..., \n",
      "   [  6.60095587e-02  -1.34764284e-01  -1.42589122e-01 ...,\n",
      "     -1.22653283e-01   1.31282613e-01  -8.88073519e-02]\n",
      "   [  6.34051263e-02  -6.53062090e-02  -1.56678166e-02 ...,\n",
      "     -1.15601614e-01   8.36638883e-02  -1.09442044e-02]\n",
      "   [  8.83337632e-02  -4.86130491e-02  -1.37281820e-01 ...,\n",
      "     -5.65669201e-02   4.32684310e-02   1.36704385e-01]]]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[[ 0.07475286  0.05369803 -0.01539745 ...,  0.07747325  0.00883024\n",
      "   0.13667034]\n",
      " [ 0.04572335  0.14844719  0.05963881 ...,  0.05697081 -0.06055739\n",
      "   0.18745105]\n",
      " [ 0.01561781 -0.19957343  0.02552973 ..., -0.12667227  0.05971188\n",
      "  -0.113671  ]\n",
      " ..., \n",
      " [-0.14538707 -0.02911191 -0.08086329 ..., -0.09525439 -0.02246072\n",
      "   0.12697719]\n",
      " [-0.08346532  0.04555551  0.04213639 ..., -0.06410145  0.10416478\n",
      "  -0.07134718]\n",
      " [ 0.03870245  0.0240592   0.05980755 ...,  0.08019627 -0.05177257\n",
      "  -0.01877305]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "[[ 0.02264589 -0.08744263  0.00968505 ...,  0.00586218 -0.03663125\n",
      "  -0.02046512]\n",
      " [-0.14679492  0.11747596 -0.0937143  ..., -0.14550458 -0.06005445\n",
      "  -0.1530145 ]\n",
      " [-0.18673617 -0.12350773 -0.12001586 ...,  0.00571698  0.03690846\n",
      "  -0.13218264]\n",
      " ..., \n",
      " [-0.11894102 -0.15060514  0.08738121 ...,  0.03179899 -0.04248327\n",
      "  -0.04089468]\n",
      " [ 0.11627183  0.03959414  0.01301557 ...,  0.04724856 -0.05326044\n",
      "  -0.00396959]\n",
      " [-0.08360618  0.13755581 -0.01291391 ...,  0.05654441 -0.01635506\n",
      "   0.02896426]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n",
      "[[ -6.06098063e-02   1.39932841e-01  -2.34195925e-02   6.58380613e-02\n",
      "    8.30256268e-02  -3.13472636e-02  -8.32143873e-02  -1.90244883e-01\n",
      "    5.86132966e-02  -7.40273818e-02  -6.75921962e-02   2.11111121e-02\n",
      "    1.94764942e-01   6.01053797e-02   1.78307518e-02   7.49125183e-02\n",
      "   -7.24332780e-02   4.57040146e-02  -5.98753579e-02   1.44136116e-01]\n",
      " [  2.30831299e-02  -3.24746184e-02   6.81489632e-02   1.49996132e-01\n",
      "   -3.69106093e-03   9.31549221e-02   4.93648183e-03   8.06323737e-02\n",
      "   -1.42348453e-01   2.93486621e-02   7.85737559e-02  -6.44409955e-02\n",
      "   -1.92836031e-01  -1.23576522e-01   6.03790991e-02  -1.62798725e-02\n",
      "   -1.51705965e-01   1.43875346e-01   9.12528299e-03   1.22533694e-01]\n",
      " [  1.30740151e-01  -3.89774702e-02   1.09302931e-01  -1.39396012e-01\n",
      "    1.28096968e-01  -1.15522230e-02  -1.67014711e-02   7.06443423e-03\n",
      "   -1.58954471e-01   1.34783268e-01  -4.28265706e-02   2.39721369e-02\n",
      "   -3.50376405e-02  -1.35198878e-02  -1.34192571e-01   1.63603067e-01\n",
      "    1.68523431e-01  -5.35146855e-02  -7.88669214e-02  -3.79598960e-02]\n",
      " [ -1.69494152e-01   1.01570927e-01  -4.23879772e-02   1.38015345e-01\n",
      "    1.75556615e-02  -5.25291860e-02   6.13583736e-02   1.13450717e-02\n",
      "   -5.61710261e-02   7.16643408e-02   3.64286564e-02  -3.73516493e-02\n",
      "   -6.51681870e-02  -5.30539267e-02  -1.63764115e-02  -1.12989031e-01\n",
      "    1.58217847e-01  -4.88452939e-03  -1.68633007e-04  -2.92083211e-02]\n",
      " [  2.81222705e-02  -3.71326320e-02  -1.88091807e-02   6.71215355e-02\n",
      "   -1.01691537e-01  -8.69269762e-03   1.39664337e-02  -2.55924789e-03\n",
      "    4.28752862e-02  -4.08900939e-02   1.51986629e-01  -8.57732147e-02\n",
      "   -1.15055941e-01  -1.96302030e-02  -7.07573518e-02  -1.69180557e-01\n",
      "   -1.50750995e-01  -5.18959872e-02  -9.92481858e-02  -1.51097570e-02]\n",
      " [  1.57594785e-01  -5.67059629e-02  -1.73523482e-02  -9.55861583e-02\n",
      "    1.40212476e-01  -9.29384083e-02   1.04673587e-01   1.04088940e-01\n",
      "    1.09278671e-01   1.62163116e-02   4.53521013e-02  -2.46194210e-02\n",
      "    1.24719001e-01   1.62874267e-01  -5.17667783e-03   1.30100057e-01\n",
      "    5.47196344e-02   4.71529625e-02  -6.06143959e-02  -1.13174915e-01]\n",
      " [ -7.08179325e-02  -2.69192960e-02   1.26003906e-01  -1.07166760e-01\n",
      "    5.85066155e-02  -1.03173010e-01   6.87823519e-02  -1.88767146e-02\n",
      "    6.58186004e-02  -4.76418063e-02   3.54070775e-02  -6.46259561e-02\n",
      "   -6.09134734e-02   5.66384196e-02  -4.70909514e-02  -2.79288087e-02\n",
      "   -3.46252993e-02   6.60079420e-02   3.23331542e-02  -6.25643805e-02]\n",
      " [ -3.24896462e-02   2.19279788e-02  -1.47677502e-02   1.27301082e-01\n",
      "    9.25795957e-02  -6.56036241e-03  -1.80357322e-01   1.09841742e-01\n",
      "    8.73763766e-03   5.17963953e-02  -7.56033808e-02   6.02642708e-02\n",
      "    3.85888033e-02  -6.67999312e-02  -1.13954894e-01   1.35909125e-01\n",
      "    2.05624532e-02  -1.40768856e-01   1.24146260e-01  -1.84604004e-02]\n",
      " [  2.07104776e-02  -1.40008181e-01  -3.63784353e-03   1.24215841e-01\n",
      "    6.62326533e-03  -1.06796674e-01   8.56203511e-02   1.61826499e-02\n",
      "   -1.67755201e-01  -3.09887051e-04  -1.70167033e-02  -4.49780636e-02\n",
      "    8.97590667e-02  -3.58652393e-03   2.17792802e-02  -5.47069386e-02\n",
      "   -1.11841060e-01  -6.46526068e-02   1.82669625e-01   1.90755352e-01]\n",
      " [ -5.63810356e-02   1.39015585e-01   9.56301577e-03   4.43780236e-02\n",
      "   -4.22113016e-02   4.58431867e-04  -1.07839741e-01   1.57947034e-01\n",
      "    1.84956878e-01  -4.14640419e-02   1.43573999e-01  -5.03324382e-02\n",
      "    3.63605171e-02  -4.45384644e-02  -1.65259093e-01   1.04653835e-01\n",
      "   -3.08440421e-02   7.84660205e-02   6.53633028e-02   1.43784806e-01]\n",
      " [  1.41875178e-01   1.02619924e-01   9.44106504e-02   7.11233914e-03\n",
      "    8.39625951e-03  -1.95996347e-03  -5.73299006e-02   5.74301295e-02\n",
      "   -5.16708456e-02   7.68215507e-02   2.56707873e-02   8.27340409e-02\n",
      "    1.82390139e-01   8.30608606e-02  -4.62257629e-03  -1.40044605e-02\n",
      "    8.34061503e-02   5.63032217e-02  -3.13478671e-02  -2.09041964e-02]\n",
      " [  6.19426034e-02  -3.73577736e-02  -1.92127749e-01   1.39160976e-02\n",
      "   -1.74903616e-01  -4.27594520e-02  -6.94778040e-02   2.74036136e-02\n",
      "    7.86581784e-02  -4.35073301e-02  -7.92151690e-03   6.71662465e-02\n",
      "    1.36981890e-01   9.86009538e-02  -4.44793850e-02  -2.60076672e-02\n",
      "    2.04366408e-02  -7.64549896e-02   2.00038645e-02  -1.77406102e-01]\n",
      " [  1.10649161e-01   1.62709802e-01   1.17050171e-01  -5.76459877e-02\n",
      "   -9.87867787e-02   6.03628121e-02  -6.57354295e-02  -3.63593958e-02\n",
      "   -4.05558310e-02  -5.47292046e-02  -9.38705355e-02   1.12942062e-01\n",
      "   -1.45709902e-01  -4.45096307e-02   9.54469964e-02   1.88672215e-01\n",
      "    1.09149694e-01  -1.09160624e-01   5.44027686e-02   6.87011182e-02]\n",
      " [  7.40158663e-04  -2.80622230e-03  -4.07298170e-02   6.98141158e-02\n",
      "   -1.09011687e-01   1.42417759e-01   6.14741445e-02  -1.63356781e-01\n",
      "    1.06470369e-01  -2.52593942e-02  -2.62056980e-02   7.32359737e-02\n",
      "    7.60226557e-03  -3.27929668e-03  -7.89213106e-02   1.17689624e-01\n",
      "   -8.77387971e-02   8.03031847e-02  -7.90562630e-02  -5.25894240e-02]\n",
      " [  2.05976013e-02  -6.70124367e-02   1.63863122e-01   1.42536256e-02\n",
      "    7.88202286e-02   6.34091124e-02  -7.67953172e-02  -2.87007689e-02\n",
      "   -1.26842394e-01  -1.88319311e-01   6.60004839e-03  -1.00189589e-01\n",
      "    8.16286430e-02   5.10489903e-02   1.17185988e-01  -2.06318423e-02\n",
      "   -1.68010458e-01  -6.95039406e-02  -1.46847174e-01  -9.96402428e-02]\n",
      " [  5.45608476e-02   6.70162365e-02  -3.74410190e-02   1.16620779e-01\n",
      "   -3.03749144e-02  -4.15440947e-02  -1.04317978e-01   1.95080396e-02\n",
      "   -9.63816326e-03   1.32958934e-01   6.41660243e-02  -1.61400706e-01\n",
      "    1.03333890e-02   4.46424037e-02  -5.26113287e-02  -1.27827704e-01\n",
      "   -1.29299155e-02  -5.59141953e-03   5.33303730e-02   7.04986323e-03]\n",
      " [ -1.08636118e-01   1.54970353e-03  -2.80620884e-02  -1.20887868e-01\n",
      "   -1.19066648e-01   6.49020597e-02  -2.23295409e-02   1.10623352e-01\n",
      "   -6.97448850e-02  -2.66401023e-02  -1.52922869e-02  -5.34516992e-03\n",
      "    5.94852231e-02  -4.29579057e-02  -1.09621808e-02   6.08001240e-02\n",
      "    4.14233655e-02   1.16687290e-01   2.44419444e-02  -8.96549597e-03]\n",
      " [ -9.19758976e-02   1.23585343e-01  -4.05547209e-03   3.33012491e-02\n",
      "   -7.71953017e-02   5.91359735e-02  -3.71106192e-02  -6.12003021e-02\n",
      "    1.65340267e-02   4.58226353e-02   1.73956856e-01   1.03307448e-01\n",
      "   -3.66939865e-02   5.28126620e-02   2.95411386e-02   7.90626332e-02\n",
      "   -2.43851375e-02   1.24157906e-01  -6.11277930e-02  -1.15284242e-01]\n",
      " [  7.07143918e-02   1.72603533e-01   9.13387747e-04   1.09135829e-01\n",
      "    1.84487626e-01   1.19955122e-01  -4.15985249e-02   3.70839350e-02\n",
      "   -1.99597225e-01  -1.82662457e-01  -5.10831587e-02  -6.77680671e-02\n",
      "    1.07087813e-01  -7.96979666e-02  -6.93253055e-02   4.08092700e-02\n",
      "   -1.51955485e-01  -1.11532427e-01   1.18486002e-01   9.06914175e-02]\n",
      " [ -1.88313331e-03   1.30433023e-01   1.35812804e-01  -1.12433314e-01\n",
      "    1.81640312e-02  -1.97989866e-01  -7.80604333e-02  -6.47659600e-03\n",
      "    1.94131006e-02  -1.01160228e-01  -1.08383872e-01  -1.85420103e-02\n",
      "   -2.00650166e-03  -1.17342949e-01  -6.90995678e-02   1.27546847e-01\n",
      "    1.74918607e-01  -1.32678017e-01   6.36435151e-02   4.43020239e-02]\n",
      " [  1.11465834e-01   4.09581661e-02   3.73346508e-02  -5.84375821e-02\n",
      "    8.94494355e-02  -1.40443683e-01   4.96710353e-02  -1.64102256e-01\n",
      "   -6.62280098e-02   1.05757706e-01   2.27827188e-02  -8.40096697e-02\n",
      "   -4.96685952e-02   4.51039225e-02  -6.61058957e-03   1.46926744e-02\n",
      "    1.76710710e-01   1.47787198e-01   5.31867221e-02   1.53630739e-02]\n",
      " [ -7.08035603e-02  -3.76729593e-02  -4.17772755e-02  -1.00326084e-01\n",
      "    4.90443446e-02   6.18116558e-02   5.45123816e-02   1.06580323e-02\n",
      "   -1.91478245e-02  -3.31947729e-02   1.50228649e-01  -2.15151925e-02\n",
      "    1.16498984e-01   6.12018779e-02  -5.13529964e-02   4.31606248e-02\n",
      "    8.96981917e-03  -8.52986723e-02   1.19851410e-01  -1.78200930e-01]\n",
      " [  9.23814923e-02  -1.78224385e-01   2.06648419e-03   5.01350574e-02\n",
      "    1.62237003e-01   4.01046723e-02   1.18638957e-02   7.74710178e-02\n",
      "   -1.91300940e-02  -4.35048230e-02   4.80448082e-02  -1.05286814e-01\n",
      "    2.27237009e-02   1.27476692e-01   2.63179909e-03  -7.41380379e-02\n",
      "   -5.89181483e-02  -1.27077535e-01  -1.62488550e-01  -3.17234360e-02]\n",
      " [ -2.61060447e-02  -1.09573916e-01   1.30343094e-01  -1.97136238e-01\n",
      "   -8.87642056e-02   6.15958758e-02   1.30948335e-01   7.30775595e-02\n",
      "    1.68565542e-01  -8.21621269e-02   3.64616103e-02   8.17610100e-02\n",
      "   -5.37670329e-02   7.15310723e-02   7.71525456e-03   2.65665036e-02\n",
      "   -2.49715820e-02   3.06783468e-02   1.72590896e-01  -1.75912678e-01]\n",
      " [ -9.76909027e-02   3.14854406e-04  -7.89864436e-02   1.98225435e-02\n",
      "   -9.30057019e-02  -9.63040665e-02   4.89312187e-02   1.40655622e-01\n",
      "   -1.21115558e-01  -5.30482596e-03  -1.52649581e-01   1.33919150e-01\n",
      "   -1.86145082e-02  -2.59107780e-02   1.39418012e-02  -4.52414453e-02\n",
      "    6.24142364e-02  -1.42002612e-01  -3.33364569e-02   1.38515523e-02]\n",
      " [  1.39673874e-01  -3.11748683e-02  -1.13040268e-01   1.90333407e-02\n",
      "   -1.39493331e-01   8.59263241e-02  -1.29634589e-01  -1.09385721e-01\n",
      "   -3.43946926e-02   1.75641850e-01   1.15401365e-01   1.39289245e-01\n",
      "    9.66017470e-02  -1.70751914e-01  -7.79743865e-02   1.57751843e-01\n",
      "    1.72730908e-01   5.52069917e-02   6.33064434e-02   1.71185836e-01]\n",
      " [ -8.00501481e-02   3.30564491e-02   1.23285070e-01  -8.63130167e-02\n",
      "   -9.27607641e-02  -8.30241963e-02  -1.90431938e-01  -3.04526035e-02\n",
      "   -3.99931408e-02  -1.56844467e-01   5.72110480e-03  -9.26305130e-02\n",
      "   -1.05423845e-01  -1.64920073e-02   2.73344573e-02  -5.62551096e-02\n",
      "    8.17438811e-02  -2.22669467e-02  -1.58374488e-01  -3.11510917e-02]\n",
      " [ -8.70876014e-02  -1.91277806e-02   2.81269997e-02   1.54393381e-02\n",
      "   -1.40198236e-02   3.75530273e-02   6.57121763e-02  -7.93031305e-02\n",
      "   -1.80396419e-02  -7.04233795e-02  -3.41289155e-02   4.50322516e-02\n",
      "    1.19481027e-01   1.09093860e-01   9.32920128e-02   1.04803205e-01\n",
      "    1.53040811e-01   5.75843565e-02   9.67813060e-02  -1.01476990e-01]\n",
      " [  1.96498454e-01   1.12350419e-01   1.38017479e-02   2.16386952e-02\n",
      "    4.20151316e-02  -3.49286571e-02   1.08228229e-01  -5.91944121e-02\n",
      "   -7.23566338e-02  -2.86395699e-02  -7.10693151e-02  -8.21878538e-02\n",
      "   -9.85859260e-02  -5.46420291e-02  -5.88571541e-02  -5.34591377e-02\n",
      "    7.69767389e-02  -5.26318923e-02   4.00788561e-02   4.50515784e-02]\n",
      " [ -1.22562125e-01  -4.00723480e-02  -1.40778825e-01   1.43424317e-01\n",
      "   -2.87744049e-02  -1.12653570e-02  -9.69426781e-02  -1.29207283e-01\n",
      "   -3.79476659e-02  -5.69941103e-02  -5.38957901e-02  -1.16903864e-01\n",
      "   -7.44860843e-02  -1.45829931e-01  -2.32129432e-02  -9.66448337e-02\n",
      "    1.64574310e-01   5.78946956e-02  -6.19587302e-02  -1.90927833e-02]\n",
      " [  1.40925795e-01   2.93142702e-02  -9.62442905e-02   7.06529617e-02\n",
      "   -2.41140928e-02   3.15621570e-02  -4.46410254e-02   1.22067533e-01\n",
      "    9.85628292e-02  -1.77201807e-01   7.08118752e-02  -1.48507103e-01\n",
      "    8.49613622e-02   1.26136288e-01  -1.27116954e-02   6.75868317e-02\n",
      "   -1.73801214e-01   1.82366356e-01   3.30021307e-02  -4.78626788e-02]\n",
      " [  9.68571827e-02   7.42467791e-02  -1.68000143e-02   2.14538891e-02\n",
      "    7.13724717e-02  -5.34362867e-02  -1.15486659e-01  -3.31843346e-02\n",
      "    1.23536224e-02   6.21245801e-02   1.42414048e-01  -1.65864319e-01\n",
      "   -1.11425696e-02  -8.49339366e-02  -2.49353945e-02   1.23264551e-01\n",
      "    8.84955674e-02   1.25345692e-01   4.65590544e-02  -1.39238779e-02]\n",
      " [ -1.14527762e-01  -3.36437337e-02  -3.61451991e-02   1.23875789e-01\n",
      "    1.05116189e-01  -1.24824885e-02  -1.15862563e-01   1.55289993e-02\n",
      "   -6.09055571e-02   1.79067012e-02   3.92885646e-03  -8.35481957e-02\n",
      "   -1.71749927e-02   1.01837456e-01  -4.97318842e-02   1.37673244e-01\n",
      "    3.02592553e-02  -7.24980757e-02  -8.50470588e-02  -3.92701142e-02]\n",
      " [  3.85389552e-02  -9.95091423e-02   1.87789705e-02   9.64075234e-03\n",
      "    1.05246857e-01  -7.37827867e-02  -1.06021084e-01  -1.94557384e-01\n",
      "    1.32721111e-01   1.71754673e-01  -3.98649089e-02   1.31131664e-01\n",
      "   -1.03922725e-01  -7.31485039e-02   1.81158736e-01   1.19658194e-01\n",
      "   -1.34017587e-01  -2.58157998e-02  -6.06150180e-03   3.42217311e-02]\n",
      " [  7.10800756e-03   1.43748701e-01   5.64973131e-02  -1.41382560e-01\n",
      "   -8.69399086e-02  -1.00713216e-01  -1.68099087e-02   3.63041721e-02\n",
      "   -2.23568361e-02   4.17773984e-02  -8.13464597e-02   1.42547116e-01\n",
      "    2.84157833e-03  -1.76726095e-02  -7.46878907e-02  -3.05172093e-02\n",
      "    1.17838429e-02   8.10532421e-02  -8.76394436e-02  -5.03256582e-02]\n",
      " [ -5.76965176e-02   1.15196155e-02   1.08148471e-01   9.04376879e-02\n",
      "    7.50928465e-03  -3.72612365e-02  -7.09079846e-04  -1.11057736e-01\n",
      "   -2.71251891e-02  -1.34112343e-01   7.69796893e-02   1.50421411e-01\n",
      "    1.52829990e-01  -6.35342523e-02   5.20135760e-02   1.17689155e-01\n",
      "   -1.23294905e-01   6.34301677e-02   6.06730245e-02  -6.73918352e-02]\n",
      " [ -1.82963684e-02   1.02002241e-01  -7.28741735e-02  -4.83446829e-02\n",
      "    8.95678326e-02   6.84166476e-02  -6.42513409e-02   5.81748001e-02\n",
      "    6.66353554e-02  -2.27431506e-02  -1.63116992e-01   3.03469952e-02\n",
      "   -1.62016712e-02  -2.39967536e-02  -5.71606755e-02   4.69354279e-02\n",
      "   -5.85251451e-02   1.43214166e-01  -1.68039307e-01  -9.58141685e-03]\n",
      " [ -1.43756017e-01  -4.80685495e-02  -7.27787688e-02   1.75266966e-01\n",
      "   -5.91949187e-02   2.81570498e-02   7.00033605e-02   2.31082160e-02\n",
      "   -1.11923091e-01   4.59559336e-02  -1.11752272e-01  -6.94105253e-02\n",
      "    4.55234423e-02  -1.74549431e-01  -4.36101370e-02  -1.20650806e-01\n",
      "    1.73515201e-01   1.12365142e-01   4.07192856e-03   5.42421415e-02]\n",
      " [ -4.78102043e-02   4.61574383e-02   1.30634949e-01  -3.51185761e-02\n",
      "    6.04803227e-02  -5.53136170e-02   9.53760669e-02  -7.96132255e-03\n",
      "    6.26936555e-02   6.11342788e-02   1.50273917e-02  -4.11268882e-02\n",
      "    1.01130679e-01  -8.24644938e-02  -1.08973989e-02   9.23501328e-03\n",
      "   -7.09973425e-02  -1.53584629e-01   3.58737335e-02   1.05463136e-02]\n",
      " [  6.72674924e-02   3.75913382e-02  -6.49633855e-02   4.54267487e-02\n",
      "    5.70310168e-02  -9.58436728e-02   2.23176349e-02  -2.66787745e-02\n",
      "    2.11155638e-02  -8.43421649e-03   9.69620273e-02  -4.25605886e-02\n",
      "    5.53752556e-02  -7.20048836e-03   6.81897253e-02   8.17821994e-02\n",
      "    4.09257673e-02   7.28141218e-02  -5.09503447e-02   6.56161308e-02]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[[ 0.05322433 -0.00213061 -0.01887153 -0.14792131 -0.01396862  0.11247244\n",
      "   0.07515582  0.03644124 -0.04107055  0.03658975]\n",
      " [ 0.07046854  0.03580694  0.07659318  0.01651814 -0.02510293 -0.04294583\n",
      "  -0.06555348 -0.01606617 -0.13949005  0.01910124]\n",
      " [ 0.06970679 -0.10475858 -0.17657332  0.03868675 -0.1276506  -0.04671553\n",
      "  -0.03264581  0.04273702  0.00221791 -0.12343247]\n",
      " [-0.03169028 -0.03345538 -0.13353285  0.14057784 -0.00899817  0.0426076\n",
      "  -0.01514081  0.0038318  -0.05874484 -0.04067173]\n",
      " [-0.00137939  0.12178852  0.00241863  0.05236092  0.1376826  -0.09608154\n",
      "  -0.02633854 -0.11512681  0.17953391  0.02595334]\n",
      " [ 0.01101883 -0.00698618  0.03892886 -0.02808859  0.09389086  0.03939118\n",
      "   0.08095436  0.0505697   0.06361509 -0.005931  ]\n",
      " [ 0.05610298  0.02359715 -0.11503929 -0.01024634 -0.1892024  -0.00202251\n",
      "   0.00187554 -0.19183052 -0.11188742 -0.0512691 ]\n",
      " [ 0.04863361  0.03588656  0.05604795 -0.0905247   0.0105998   0.02169206\n",
      "  -0.0135945   0.03022494  0.01335619  0.02570581]\n",
      " [ 0.00705122 -0.02119004 -0.17369068  0.0690172  -0.11327553 -0.09349993\n",
      "   0.18240285 -0.02190466 -0.01713855  0.00589733]\n",
      " [ 0.16282167 -0.02391767 -0.18211512 -0.02265077  0.04106408 -0.06500766\n",
      "   0.12583901 -0.01142842  0.09929951 -0.16057575]\n",
      " [-0.00622317  0.03306099 -0.14772356  0.06710266 -0.0074051  -0.09532217\n",
      "  -0.11261125  0.1881649  -0.0697284   0.04995602]\n",
      " [-0.09825899  0.05204666  0.05498106  0.0041994  -0.0167067   0.1118171\n",
      "   0.19266604  0.0797207  -0.1008634  -0.09991863]\n",
      " [ 0.0910027   0.0182782   0.01657648 -0.04214547 -0.04073866 -0.01742093\n",
      "  -0.16149688  0.07304665  0.03194191  0.06578562]\n",
      " [ 0.09666558 -0.09748559 -0.05521236 -0.02912927 -0.04563316 -0.06505448\n",
      "   0.10148769 -0.08590575  0.19280942 -0.05445966]\n",
      " [ 0.00698134 -0.01116121 -0.08373004 -0.02644214 -0.02137783 -0.00108655\n",
      "  -0.09105319  0.09562537 -0.08788979 -0.08000828]\n",
      " [-0.09099656  0.0286709   0.04310888 -0.14248393 -0.0466071   0.0615496\n",
      "  -0.02158532  0.02183749 -0.06802449 -0.02711098]\n",
      " [ 0.10619847 -0.03427313 -0.1711255  -0.1654859  -0.14276272  0.08817949\n",
      "   0.10065255  0.09262924 -0.14782991  0.11642208]\n",
      " [ 0.01967425  0.06128507  0.02973141  0.09059317 -0.03676913 -0.16042356\n",
      "   0.00504018 -0.09966733 -0.07000733 -0.12706399]\n",
      " [ 0.07912184  0.12899604  0.13324028  0.06725414 -0.02339008 -0.11633845\n",
      "   0.05948946 -0.06241935  0.07508095 -0.01854089]\n",
      " [ 0.08090416  0.16909753  0.04022956  0.06020873 -0.10206368  0.11172372\n",
      "   0.11349576 -0.01535752 -0.06957281  0.17693844]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#ALL VAriables and placeholders\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.tf_train_dataset,feed_dict=feed_dict))\n",
    "        print(session.run(m.tf_train_labels,feed_dict=feed_dict))\n",
    "        print(session.run(m.t1,feed_dict=feed_dict))\n",
    "        print(session.run(m.t2,feed_dict=feed_dict))\n",
    "        print(session.run(m.tt1,feed_dict=feed_dict))\n",
    "        print(session.run(m.tt2,feed_dict=feed_dict))\n",
    "        print(session.run(m.tc1,feed_dict=feed_dict))\n",
    "        print(session.run(m.tc2,feed_dict=feed_dict))\n",
    "        print(session.run(m.tcc1,feed_dict=feed_dict))\n",
    "        print(session.run(m.tcc2,feed_dict=feed_dict))\n",
    "        print(session.run(m.te1,feed_dict=feed_dict))\n",
    "        print(session.run(m.te2,feed_dict=feed_dict))\n",
    "        print(session.run(m.tee1,feed_dict=feed_dict))\n",
    "        print(session.run(m.tee2,feed_dict=feed_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[ 0.13722591 -0.42817613 -0.08117278 ...,  0.03523119  0.06313569\n",
      "  -0.00555433]\n",
      " [ 0.13632379 -0.42942616 -0.08575635 ...,  0.03557774  0.05991968\n",
      "  -0.01211819]\n",
      " [ 0.14078397 -0.43178383 -0.08107589 ...,  0.03891521  0.06366557\n",
      "  -0.01077212]\n",
      " ..., \n",
      " [ 0.13994774 -0.42433229 -0.08693504 ...,  0.03361739  0.05805166\n",
      "  -0.00692041]\n",
      " [ 0.13149042 -0.43246561 -0.09255605 ...,  0.02819347  0.06731059\n",
      "  -0.01660879]\n",
      " [ 0.1362427  -0.43015057 -0.08238041 ...,  0.03280582  0.06179554\n",
      "  -0.0125486 ]]\n"
     ]
    }
   ],
   "source": [
    "#logits\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.logits,feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.49215686]\n",
      "   [ 0.5       ]\n",
      "   [ 0.23333333]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.31176472]\n",
      "   [ 0.3392157 ]\n",
      "   [ 0.08431373]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.48823529]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.49215686]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.49215686]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.13137256]\n",
      "   [ 0.13529412]\n",
      "   [ 0.12352941]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.49607843]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.49607843]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.13137256]\n",
      "   [ 0.13529412]\n",
      "   [ 0.12352941]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[ 0.46862745]\n",
      "   [ 0.25686276]\n",
      "   [ 0.22941177]\n",
      "   ..., \n",
      "   [ 0.24901961]\n",
      "   [-0.5       ]\n",
      "   [-0.48823529]]\n",
      "\n",
      "  [[ 0.43333334]\n",
      "   [ 0.02941176]\n",
      "   [-0.02156863]\n",
      "   ..., \n",
      "   [ 0.26862746]\n",
      "   [-0.5       ]\n",
      "   [-0.48823529]]\n",
      "\n",
      "  [[-0.35882354]\n",
      "   [-0.24509804]\n",
      "   [-0.08431373]\n",
      "   ..., \n",
      "   [ 0.26862746]\n",
      "   [-0.5       ]\n",
      "   [-0.48823529]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.36274511]\n",
      "   [-0.25686276]\n",
      "   [-0.1       ]\n",
      "   ..., \n",
      "   [-0.1       ]\n",
      "   [ 0.12745099]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.45686275]\n",
      "   [ 0.21372549]\n",
      "   [ 0.17843138]\n",
      "   ..., \n",
      "   [ 0.05686275]\n",
      "   [-0.02941176]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.46862745]\n",
      "   [ 0.26078433]\n",
      "   [ 0.24117647]\n",
      "   ..., \n",
      "   [ 0.47254902]\n",
      "   [-0.2254902 ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[-0.49215686]\n",
      "   [-0.48823529]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.48823529]\n",
      "   [-0.49215686]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.48039216]\n",
      "   [-0.1509804 ]\n",
      "   ..., \n",
      "   [-0.14705883]\n",
      "   [-0.48039216]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.28431374]\n",
      "   [ 0.23333333]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.24117647]\n",
      "   [-0.26862746]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.5       ]\n",
      "   [ 0.49607843]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [ 0.49215686]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]]\n",
      "\n",
      "  [[ 0.10392157]\n",
      "   [ 0.46470588]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.40588236]\n",
      "   [-0.00980392]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.28431374]\n",
      "   [ 0.25686276]\n",
      "   ..., \n",
      "   [ 0.12745099]\n",
      "   [-0.37843138]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[-0.4137255 ]\n",
      "   [-0.11960784]\n",
      "   [ 0.25294119]\n",
      "   ..., \n",
      "   [ 0.2372549 ]\n",
      "   [-0.0882353 ]\n",
      "   [-0.43333334]]\n",
      "\n",
      "  [[ 0.40980393]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.49607843]\n",
      "   [ 0.5       ]\n",
      "   [ 0.19019608]]\n",
      "\n",
      "  [[ 0.5       ]\n",
      "   [ 0.49215686]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [ 0.28823531]\n",
      "   [ 0.02156863]\n",
      "   [-0.3509804 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.5       ]\n",
      "   [ 0.49607843]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.28431374]\n",
      "   [ 0.48431373]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.48823529]\n",
      "   [-0.2647059 ]\n",
      "   [ 0.13137256]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.44117647]\n",
      "   [ 0.30784315]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.48823529]\n",
      "   [ 0.5       ]\n",
      "   [ 0.01764706]]\n",
      "\n",
      "  [[-0.12352941]\n",
      "   [ 0.5       ]\n",
      "   [ 0.48431373]\n",
      "   ..., \n",
      "   [-0.09215686]\n",
      "   [-0.27254903]\n",
      "   [-0.42941177]]\n",
      "\n",
      "  [[-0.42941177]\n",
      "   [-0.30000001]\n",
      "   [-0.13921569]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]]\n",
      "[[[[-0.08039216]\n",
      "   [ 0.0254902 ]\n",
      "   [ 0.02156863]\n",
      "   ..., \n",
      "   [ 0.02156863]\n",
      "   [ 0.0254902 ]\n",
      "   [-0.08039216]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.49215686]\n",
      "   [-0.49215686]\n",
      "   [-0.48039216]\n",
      "   ..., \n",
      "   [-0.48039216]\n",
      "   [-0.49215686]\n",
      "   [-0.49215686]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.49215686]\n",
      "   [-0.49215686]\n",
      "   [-0.48039216]\n",
      "   ..., \n",
      "   [-0.48039216]\n",
      "   [-0.49215686]\n",
      "   [-0.49215686]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.08039216]\n",
      "   [ 0.0254902 ]\n",
      "   [ 0.02156863]\n",
      "   ..., \n",
      "   [ 0.02156863]\n",
      "   [ 0.0254902 ]\n",
      "   [-0.08039216]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.49215686]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.48823529]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.03333334]\n",
      "   [ 0.5       ]\n",
      "   [ 0.48431373]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.18235295]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.39411765]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49607843]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.05294118]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [-0.49215686]\n",
      "   [-0.49607843]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.08039216]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [-0.45294118]\n",
      "   [-0.5       ]\n",
      "   [-0.49607843]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.0882353 ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.22941177]\n",
      "   [-0.46470588]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.18235295]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [-0.19019608]\n",
      "   [-0.5       ]\n",
      "   [-0.48823529]]\n",
      "\n",
      "  [[ 0.27254903]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49215686]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.48823529]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[ 0.35882354]\n",
      "   [ 0.5       ]\n",
      "   [ 0.49607843]\n",
      "   ..., \n",
      "   [-0.48823529]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.49215686]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.48823529]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.15490197]\n",
      "   [ 0.19019608]\n",
      "   [-0.30000001]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.28431374]\n",
      "   [-0.48039216]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [-0.22156863]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.49215686]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]\n",
      "\n",
      "  [[-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   ..., \n",
      "   [-0.5       ]\n",
      "   [-0.5       ]\n",
      "   [-0.5       ]]]\n",
      "\n",
      "\n",
      " [[[ 0.18627451]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [-0.04117647]]\n",
      "\n",
      "  [[ 0.18235295]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [-0.02156863]]\n",
      "\n",
      "  [[ 0.13921569]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [-0.01764706]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.02941176]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.30784315]]\n",
      "\n",
      "  [[-0.02941176]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.3392157 ]]\n",
      "\n",
      "  [[-0.04117647]\n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   ..., \n",
      "   [ 0.5       ]\n",
      "   [ 0.5       ]\n",
      "   [ 0.33529413]]]]\n"
     ]
    }
   ],
   "source": [
    "#constants\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.tf_valid_dataset,feed_dict=feed_dict))\n",
    "        print(session.run(m.tf_test_dataset,feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[ 0.2734721  -0.24156116  0.03673472 ..., -0.17231134 -0.18172911\n",
      "   0.17072716]\n",
      " [ 0.27202427 -0.23231871  0.03885259 ..., -0.16558921 -0.17832908\n",
      "   0.1722438 ]\n",
      " [ 0.27282059 -0.23083945  0.03932489 ..., -0.1647839  -0.18193199\n",
      "   0.17909953]\n",
      " ..., \n",
      " [ 0.26842979 -0.22426018  0.04287065 ..., -0.16182539 -0.17790541\n",
      "   0.17686117]\n",
      " [ 0.27265045 -0.22740681  0.0424087  ..., -0.16399659 -0.17930838\n",
      "   0.17471737]\n",
      " [ 0.27373245 -0.23989606  0.03883404 ..., -0.17229563 -0.18083332\n",
      "   0.16547903]]\n",
      "[[ 0.27263662 -0.2322666   0.0365146  ..., -0.16669786 -0.17489615\n",
      "   0.16796021]\n",
      " [ 0.27858487 -0.24485722  0.04403922 ..., -0.1772857  -0.1838752\n",
      "   0.1673573 ]\n",
      " [ 0.26988605 -0.22851013  0.04470102 ..., -0.16523649 -0.18059248\n",
      "   0.17473324]\n",
      " ..., \n",
      " [ 0.2679964  -0.2362799   0.04072398 ..., -0.16913608 -0.1854147\n",
      "   0.17510574]\n",
      " [ 0.2711221  -0.22989376  0.03971886 ..., -0.16572709 -0.17731711\n",
      "   0.17213301]\n",
      " [ 0.26967674 -0.22570693  0.0418191  ..., -0.16048542 -0.18191397\n",
      "   0.18329772]]\n"
     ]
    }
   ],
   "source": [
    "#constants\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.valid_network,feed_dict=feed_dict))\n",
    "        print(session.run(m.test_network,feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "2.31098\n"
     ]
    }
   ],
   "source": [
    "#loss\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.loss,feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[ 0.08935566  0.12448357  0.1062216  ...,  0.10395027  0.09486914\n",
      "   0.07748746]\n",
      " [ 0.09002438  0.1248565   0.10563038 ...,  0.10408387  0.095392\n",
      "   0.07795174]\n",
      " [ 0.0893115   0.12442534  0.10649552 ...,  0.10392871  0.09490911\n",
      "   0.0773934 ]\n",
      " ..., \n",
      " [ 0.08933998  0.12444712  0.10656527 ...,  0.10452928  0.09507175\n",
      "   0.0776389 ]\n",
      " [ 0.08948349  0.12468912  0.10687196 ...,  0.10439339  0.09515715\n",
      "   0.07768179]\n",
      " [ 0.08985561  0.12476162  0.10621189 ...,  0.1040139   0.09557515\n",
      "   0.07781848]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[ 0.08996077  0.12422373  0.10664456 ...,  0.10452705  0.09514491\n",
      "   0.07785553]\n",
      " [ 0.08964324  0.12439188  0.10628863 ...,  0.1044      0.09535608\n",
      "   0.0776444 ]\n",
      " [ 0.08993725  0.12439105  0.10638526 ...,  0.10436641  0.09525903\n",
      "   0.07780418]\n",
      " ..., \n",
      " [ 0.08952632  0.12436769  0.10749325 ...,  0.10485344  0.0950206\n",
      "   0.07759295]\n",
      " [ 0.08958885  0.12497854  0.1060167  ...,  0.10381569  0.0952881\n",
      "   0.07761133]\n",
      " [ 0.08998492  0.12490772  0.1056952  ...,  0.1047455   0.09538106\n",
      "   0.07774767]]\n",
      "[[ 0.08953892  0.12486833  0.10598857 ...,  0.1036755   0.09495963\n",
      "   0.07774758]\n",
      " [ 0.08943103  0.1249844   0.10642459 ...,  0.103858    0.09506232\n",
      "   0.0776742 ]\n",
      " [ 0.08978929  0.1243432   0.10708989 ...,  0.10464965  0.09537135\n",
      "   0.07768847]\n",
      " ..., \n",
      " [ 0.0901252   0.12513487  0.10584246 ...,  0.10416357  0.09552551\n",
      "   0.07799961]\n",
      " [ 0.08937595  0.12459566  0.10691568 ...,  0.1045738   0.09504613\n",
      "   0.07769644]\n",
      " [ 0.08926271  0.12456253  0.10701527 ...,  0.10493841  0.09525003\n",
      "   0.07750946]]\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.train_prediction,feed_dict=feed_dict))\n",
    "        print(session.run(m.train_pred_cls,feed_dict=feed_dict))        \n",
    "        print(session.run(m.valid_prediction,feed_dict=feed_dict))\n",
    "        print(session.run(m.test_prediction,feed_dict=feed_dict))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0.132812\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 1\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    summary_writer = tf.summary.FileWriter(log_path, session.graph) \n",
    "    all_summaries = tf.summary.merge_all() \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "   \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        print(session.run(m.acc_op,feed_dict=feed_dict))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.317532\n",
      "Minibatch accuracy: 14.06%\n",
      "Validation accuracy: 10.0%\n",
      "Duration: 0.126 sec\n",
      "Minibatch loss at step 1000: 0.380012\n",
      "Minibatch accuracy: 87.50%\n",
      "Validation accuracy: 88.5%\n",
      "Duration: 0.120 sec\n",
      "Minibatch loss at step 2000: 0.314347\n",
      "Minibatch accuracy: 89.84%\n",
      "Validation accuracy: 89.9%\n",
      "Duration: 0.139 sec\n",
      "Minibatch loss at step 3000: 0.267827\n",
      "Minibatch accuracy: 92.97%\n",
      "Validation accuracy: 90.5%\n",
      "Duration: 0.112 sec\n",
      "Minibatch loss at step 4000: 0.248602\n",
      "Minibatch accuracy: 92.19%\n",
      "Validation accuracy: 90.8%\n",
      "Duration: 0.129 sec\n",
      "Minibatch loss at step 5000: 0.247150\n",
      "Minibatch accuracy: 91.41%\n",
      "Validation accuracy: 90.8%\n",
      "Duration: 0.121 sec\n",
      "Minibatch loss at step 6000: 0.235164\n",
      "Minibatch accuracy: 95.31%\n",
      "Validation accuracy: 90.9%\n",
      "Duration: 0.114 sec\n",
      "Minibatch loss at step 7000: 0.249656\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 91.2%\n",
      "Duration: 0.108 sec\n",
      "Minibatch loss at step 8000: 0.174338\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 91.2%\n",
      "Duration: 0.109 sec\n",
      "Minibatch loss at step 9000: 0.205279\n",
      "Minibatch accuracy: 93.75%\n",
      "Validation accuracy: 91.2%\n",
      "Duration: 0.117 sec\n",
      "Test accuracy: 96.0%\n",
      " \n",
      "The duration of the whole training with 10000 steps is 1190.97 seconds,\n",
      "which is equal to:  0:0:19:50 (DAYS:HOURS:MIN:SEC)\n",
      " \n",
      "logs/19-04-2017_13-24-47\n"
     ]
    }
   ],
   "source": [
    "#full_test\n",
    "log_basedir = 'logs'\n",
    "run_label = time.strftime('%d-%m-%Y_%H-%M-%S') #e.g. 12-11-2016_18-20-45\n",
    "log_path = os.path.join(log_basedir,run_label)\n",
    "m = CNNModel()\n",
    "num_steps = 10000\n",
    "\n",
    "initial_time = time.time()\n",
    "\n",
    "with tf.Session(graph=m.graph) as session:\n",
    "    summary_writer = tf.summary.FileWriter(log_path, session.graph) \n",
    "    all_summaries = tf.summary.merge_all() \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {m.tf_train_dataset : batch_data, m.tf_train_labels : batch_labels}\n",
    "        \n",
    "        #we are going to run the session and count the duration of the running\n",
    "        start_time = time.time()\n",
    "        _, l, predictions, acc, summary = session.run(\n",
    "          [m.optimizer, m.loss, m.train_prediction, m.acc_op, all_summaries], feed_dict=feed_dict)\n",
    "        duration = time.time() - start_time\n",
    "       #writing the log\n",
    "        summary_writer.add_summary(summary,step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "        #Printing an overwiew\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.2f%%\" % (acc*100))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            m.valid_prediction.eval(), valid_labels))\n",
    "            print('Duration: %.3f sec' % duration)\n",
    "            #print_test_accuracy()\n",
    "\n",
    "    #after the loop compair our model with the test dataset    \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(m.test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "general_duration = time.time() - initial_time\n",
    "sec = timedelta(seconds=int(general_duration))\n",
    "d_time = datetime(1,1,1) + sec\n",
    "print(' ')\n",
    "print('The duration of the whole training with % s steps is %.2f seconds,'\\\n",
    "      % (num_steps,general_duration))\n",
    "print(\"which is equal to:  %d:%d:%d:%d\" % (d_time.day-1, d_time.hour, d_time.minute, d_time.second), end='')\n",
    "print(\" (DAYS:HOURS:MIN:SEC)\")\n",
    "print(' ')\n",
    "print(log_path)\n",
    "#!tensorboard --logdir=!!!copy log_path here!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard b'41' on port 6006\n",
      "(You can navigate to http://127.0.1.1:6006)\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/usr/local/bin/tensorboard\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/tensorboard/tensorboard.py\", line 151, in main\n",
      "    tb_server.serve_forever()\n",
      "  File \"/usr/lib/python3.5/socketserver.py\", line 232, in serve_forever\n",
      "    ready = selector.select(poll_interval)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir=logs/19-04-2017_13-24-47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
